Dear Mr. Allais:

Manuscript ID JFP-2019-0033 entitled "A Type and Scope Safe Universe of Syntaxes with Binding: Their Semantics and Proofs" which you submitted to the Journal of Functional Programming, has been reviewed.  The comments of the referees are included at the bottom of this letter.

The referees have suggested some minor revisions to your manuscript.  Therefore, I invite you to respond to the comments and revise your manuscript.

Specifically:

- Reviewer 1 raises some questions that the authors may want to address directly in the text, or in their response letter.
Also, the reviewer asks for the updated supplementary material, so the authors should make sure it is available and properly linked from the paper.

- Reviewer 2 suggests a number of lightweight suggestions for improvements, which should be taken into account.

- Reviewer 3 also points to several venues for improvements. In particular, I would like to emphasize the request for additional high-level explanations to make the paper more easily accessible to a wider FP audience.


To revise your manuscript, log into https://mc.manuscriptcentral.com/jfp_submit and enter your Author Center, where you will find your manuscript title listed under "Manuscripts with Decisions."  Under "Actions," click on "Create a Revision."  Your manuscript number has been appended to denote a revision.

You will be unable to make your revisions on the originally submitted version of the manuscript.  Instead, please upload your revised manuscript through your Author Center.

When submitting your revised manuscript, you will be able to respond to the comments made by the referees in the space provided.  You can use this space to document any changes you make to the original manuscript.  In order to expedite the processing of the revised manuscript, please be as specific as possible in your response to the referees.

IMPORTANT:  Your original files are available to you when you upload your revised manuscript.  Please delete any redundant files before completing the submission.

Because we are trying to facilitate timely publication of manuscripts submitted to the Journal of Functional Programming, your revised manuscript should be uploaded as soon as possible.  If it is not possible for you to submit your revision in a reasonable amount of time, say six weeks, we may have to consider your paper as a new submission. Please let me know if this timescale is not feasible.

Once again, thank you for submitting your manuscript to the Journal of Functional Programming and I look forward to receiving your revision.

Sincerely,

Éric Tanter
Journal of Functional Programming
jfp-icfp2018@dcc.uchile.cl

Referees' Comments to Author:
Referee: 1

Comments to the Author

This article is a much extended version of the ICFP'18 article of the
same title. The article exposes a generic construction of scope-safe and
(if desired/applicable) type-safe syntaxes with bindings along with
their transformation and proofs about those transformations. The paper
is very well organized and pleasant to read through: we learn about
scope safety using a de Bruijn encoding of binding, the construction of
a datatype universe of scope-safe syntaxes, the ability to index these
syntaxes with sorting/typing information and a quite minimal general
semantics for such syntaxes, taking seriously into account the meaning
of binding. The paper then goes on to present a host of enlightening
example uses of this framework (not part of the ICFP article), leading
to an almost complete compiler chain when instantiated at a toy but
appropriately chosen language (STLC with or without Lets). The framework
can indeed elegantly handle parsing, proof-producing elaboration,
bidirectional type-checking, non-trivial compiler optiomisations (global
or selective inlining of lets here) and their fusing.  The authors
present *generic* proofs about arbitrary simulations so the fusion lemma
they get is also reusable. In addition, the authors present how the
technique lifts to infinite terms and an elegant generic
equality decider for syntaxes, up-to parameter types appearing in a
given user-defined syntax.

The whole development relies on different lines of work
(datatype-generic programming, scope-safe binding, (dependent)
(co-)monadic programming, denotational semantics) and puts them together
in a seductively coherent way, paying due respect to the contributions
of earlier work. Besides, the authors are very upfront about the current
liminations of the system (e.g. the flat binding structure) and provide
convincing evidence that the current framework will stay at the basis of
future endeavors. Everything is mechanized in Agda with only one
well-delimited use of an unsafe feature for the nbe example, so I trust
the results entirely. I hence recommend its acceptance with minor
modifications / additions.

Main remarks / questions
------------------------

- You note in section 3.1 that renaming and substitution can be
  generically derived for all your syntaxes, relying on the notion of
  Thinability and VarLike values. To mechanize this, you choose to
  profit from the strict monoidal structure of arrow types to define the
  Thinning notion. A simlar choice is made in the sigma-calculus
  presentation of Shaëffer et al (ITP 2015), where renamings are `nat ->
  nat` functions and substitutions `nat -> term` functions and one needs to show extensionality properties of functions using them. Later on in
  the conclusion you mention Kaiser, Schaëffer and Starks's remark that
  assuming functional extensionality, all traversals of ACMM are stable
  under renaming. Couldn't the extensionality requirement be dropped by
  considering a different representation of environments etc... using
  finite support functions (represented as finite tuples?). Especially
  in the case considered in this article, as all variables are
  well-scoped (_-Scoped), there must be a finite number of free variables
  in any situation. Of course one could also rely on an OTT/HoTT-like
  theory with functional extensionality built-in and not bother with this.
  Maybe the point you make is also about higher-order functions?

- Suspended substitutions are hard, but I'm missing the other direction:
  generic implementations of lifting and substitution in the more
  space-efficient fashion which does not carry a substitution everywhere
  but only a lifting index and does the de Bruijn arithmetic at the
  variable sites only. It is easy to implement on untyped syntax, but it
  seems to me that the VarLike requirements prevent it in the general
  framework you present (of course, if it doesn't prevent it, I would be
  glad to know how to do it). As I understand it, the way semantics are
  defined, an application of renaming will switch between PHOAS-like
  (Kripke) and concrete (interpretation of a description) presentations
  of the term. IIUC, in the printing example, `reify` takes a Kripke
  function built from an existing concrete term, and applies it to built
  a new concrete term by combining it with the action of fresh, hence
  producing fresh names for the bound variables, already knowing about
  the names of the free variables computed with `(base vl^MName)`.  I'm
  curious what the computational content of these manipulations is and
  if the back-and-forth of going through Kripke function spaces is
  efficient or can be optimized? How would the renaming function you get
  through Semantics compare with a straightforward recursive
  implementation applying a lifting to the current renaming at each
  binder and with a function accumulating the lifting index (or types)
  and applying it in one go at the var leaves only (if that is indeed
  definable).

- The obvious question that is not directly answered in future work is:
  would this lift to a dependently-typed language easily? The closure
  under products issue is the first obstacle I guess, and it could use
  an explanatory example. Probably this also links to the unrestricted
  variable sorting issue.

Presentation:

I guess you want figure 32 to appear on page 19, not 20.

p21. "where assigned"
p21. "the two first" -> "the first two"?
p22. The definition of `base` is missing.
p38. congruence rule for c -> s.
  generic generic

Kaiser Shaëffer and Stark: relation of ren-subst and the complete
sigma-calculus?

p49. "on all of them" missing .

The pdf I got has all his links broken (tried with Skim and Preview on
MacOS X). I couldn't get access to the supplementary material here (no
link or attached file). Probably there is some already for the ICFP
article but the one for this submission ought to be larger and it would
be useful to compare the two articles.


Referee: 2

Comments to the Author
Summary
----------------

This paper presents a generic representation of intrinsically well-scoped and well-typed syntax with binders. Similarly to earlier work on generic representations of datatypes, each concrete syntax is defined by a given *description* in a universe. Making such descriptions explicit allows defining certain syntactic extensions once and for all for all syntaxes, such as adding let-bindings. It also allows defining operations on syntax generically, as well as *proving* facts about them once and for all. For example, the paper shows how to define inlining of let-binding generically for all syntaxes. The paper continues by defining a generic notion of semantics of a syntax, and gives several examples such as renaming, substitution, evaluation, pretty-printing, and type-checking. The paper also presents how to use a non-standard semantics of syntax with binders to represent cyclic data structures such as graphs. Finally, the paper presents a general framework for proving fusion lemmas that are often required when defining transformations of intrinsically well-scoped (or well-typed) syntax. The whole paper is a literate Agda script, so everything in the paper is part of actual runnable code.

Review
------------

This paper is basically a master class in dependently typed programming. It didactically builds from simple examples to the fully general definition of the universe of syntaxes, and provides a wealth of useful tools to traverse and manipulate these syntaxes. It is also practical, as it provides a library that can be used immediately to define a wide variety of syntax and get all these tools for free, as well as proofs about them.

The writing style is friendly and welcoming, providing a good (though challenging) introduction to newcomers. The code makes tasteful use of Agda's extensive syntactic flexibility to assign readable types to the (sometimes heavy) concepts.

Although not an intrinsic problem of the work in this paper, one drawback is that defining a syntax through its encoding in the universe is harder than defining the syntax directly. Clearly this initial cost is worth it since we can get all kind of generic functions and proofs from definining the encoded version. However, it may turn off some potential users of the library. I cannot help but wonder whether there is some way to get the best of both worlds, either by leveraging the power of Agda's reflection machinery or by moving to a different language with better support for encoded datatypes (if/when such a language exists).

Since the quality of the work in this paper is quite self-evident, I will focus the remainder of this review on opportunities for further improvement.

- Since the 'variable' feature of Agda is quite new and the paper makes heavy use of it, perhaps it would help to add a short explanation of which variable declarations are used (e.g. "We use σ for variables of type Type and Γ for variables of type List Type").
- For the scope checker in section 7.2, the error messages provide additional information as to what went wrong, but this is not the case for the typechecker in section 7.3 or the elaborator in section 7.4. Is there a particular reason for using Maybe instead of a more informative Either here?
- Since the paper is quite long, it would help the reader to provide pointers to where concepts are defined, especially when it is more than one section ago since they were used. For example, 'case' from section 5 is used in the definition of UnLet in section 7.5, and 'closed' from section 6.2 is used in 7.4.
- In the definition of elaboration in section 7.4, the letter Γ is used both for a List of modes and for a Typing. Although both are similar, it would be less confusing if they would use a different symbol.
- It feels a bit silly to refer to your own 2017 paper in 3rd person.
- In Figure 48, I didn't find where 'Elaborate' is defined. If it is omitted from the paper intentionally, it would help to give at least its type signature.
- For the generic version of NbE in section 7.7, currently the positivity checker has to be disabled. However (as the paper also notes) this can destroy the soundness of Agda's theory. Thus I wonder whether it would be possible to provide a safe (but still generic) version of NbE by using either a Delay monad or define a suitable domain directly using domain theory.
- Currently the paper requires some knowledge of Agda's standard library to understand completely. For example, I had to look up the definition of Rel and rel which are used in section 9.1. It would improve the paper if it either explained these concepts or at least referred to the modules of the standard library where they are defined. 
- I did not understand some of the terminology used in section 9.1, e.g. what does it mean for two objects to be 'synchronized'? And what does it mean for two environments to be 'extensionally equal'?
- The POPLMark challenge is referenced on page 42, but it is mentioned already two pages earlier.
- The sentence in section 10.3 ending with "... to go further in developing machine-checked such implementations and proofs" looks weird, maybe it's better to reword it in some other way?
- In the related work on alternative approaches via code generation (section 10.4), it would be interesting to also compare to the ott tool for mechanized metatheory.
- There's a typo in section 11: the period at the sentence ending with "acting on all of them" is missing.
- The usage of the word "preliminary" in the last paragraph of section 11.1 feels wrong; I would reword this to 'precomposition with a renaming'.
- Some of the capitalization in the references is off, e.g. Miniagda, Poplmark, coq.


Referee: 3

Comments to the Author
Syntax with binding is a recurring difficulty in the implementation and formalization of programming languages. There are a number of techniques, and each is unsatisfactory in some key way. For instance, de Bruijn indices require complicated arithmetic during substitution, and high-order approaches admit exotic terms and violate the positivity restriction of dependently typed languages. No matter which technique is chosen, programmers need to rebuild a large amount of infrastructure "from scratch" for each new language, which introduces opportunities for subtle bugs.

This paper presents an alternative approach: syntax with binding is represented by first-order codes in a universe à la Tarski, and these codes are then interpreted into an actual binding structure. The codes themselves are a fairly standard encoding of strictly-positive functors; rather than taking a direct fixed point, they are converted into syntax with binding by using a free monad construction. Having done this, it becomes possible to define completely generic operations on any describable syntax with binding, such as renaming and substitution. Because the universe of descriptions is closed under sums, it is also possible to design pieces of syntax that can be mixed and matched - the paper provides the example of a let operation that can be freely added to a given language, as well as a desugaring operation that substitutes let-bound expressions for their variables. A general notion of a semantics for a described language is capable of describing a large variety of interpretations.

The conference version was already convincing that the technique is interesting and valuable, and the greatly increased number of examples in this version of the paper is even further evidence that the technique can be used for many interesting tasks in representing programming languages. It's still unclear to what extent the technique scales up to descriptions of realistic programming languages - the associated code repository does not contain anything larger than a bidirectional simply-typed lambda calculus. But, even though it would be useful to see how well the technique worked for a language on the scale of ML or Haskell, the current demonstration shows the essential usefulness and elegance of the idea.

However, there are some issues with the presentation.

First off, polish issues: there are a number of typos and misspellings - it would have benefited from at least the use of a spell checker. The placement of code sample figures, and the haphazard selection of fonts and capitalization conventions in their captions, suggests that last-minute formatting changes were made. The level of polish is not yet up to the standards of JFP, though a fairly minor revision could easily correct all of these issues.

Aside from these more minor concerns, there are also a number of missed opportunities to explain the core ideas to a wider audience who are not necessarily experts in the current version of Agda. The paper claims to describe ideas that are widely applicable, after all. A large part of the paper is a walk-through and description of a collection of Agda programs, but the programs are illustrated only with their source code. Readers must mentally type-check and run the programs, and the highly abstracted nature of the code was not easy for me to read. The paper could be made much more accessible to the broad functional programming community by adding higher-level descriptions of the generic operators, especially visual descriptions or figures. Now that there is not a conference page limit, including these kinds of things should be easier. Basically: there's a great many trees, but little discussion of the forest.

The online version of the code uses Agda's "variable" feature to assign types to free occurrences of names. It would also be useful to include these as a table in the paper, so that uses of sigma, Gamma, I, and the like could be looked up.


Detailed comments:

p.1: There should be an indication that the paper uses color on the first page, so that readers make sure to use an appropriate printer.

p.1: "Capture avoiding substitution" should be "capture-avoiding substitution" (also search the rest of the file)

p.1: "well-scopedness" is missing the hyphen. This is an issue throughout the article - please do a search.

p.2: "ill-scoped" and "ill-typed" are missing their hyphens

p.2: "Related Work" should not be capitalized, as it's not used as a proper noun here

p.2: The paragraph beginning with "Breaking the task down" refers to an implementor, also referred to with the pronoun "they". The paragraph right after the numbered list, that begins with "Even after", uses the pronoun "you" to refer to the implementor. Consistency is necessary, and "they" seems better here.

p.2: The citation of Benton et al is in parentheses, but it's used as an ordinary noun phrase and thus should not be parenthesized.

p.2, last line: "Fusable" should probably be "fusible"

p.3: This page states that the techniques are language-independent, requiring only inductive families, but it seems to me that the use of sized types is a key part of making the fixed point construction go through. Is that not the case?

p.4: The line "We use Agda's" seems to be a part of the paragraph prior to the example of _-Scoped, so it should not be indented.

p.4: Here's a chance to explain the naming conventions. The _-Scoped family is named with a leading hyphen, and used in postfix position, as if it were ML, but many other types are used in prefix. How were the operators to use in postfix selected? Does the hyphen just indicate that it's postfix, or does it mean something else? It would be useful to explain these kinds of naming conventions, to help readers understand the code by understanding the conventions used.

p.4: The citation of "Martin-Löf (1982)" has far too many parentheses around it. In this case, it's a noun phrase - use \citet or the equivalent.

p.5: The turnstile notation (_⊢_) seems a bit obscure. Please consider adding an explanation of why it was chosen to help readers remember its meaning later on.

p.5: The claim that these notations are more readable is not immediately clear from this page, but it does seem to be the case later on, with the exception of the somewhat confusing turnstile notation.

p.5: In the definition of Var, small sigma occurs free. This is usually not allowed in Agda. In the online code, this is done with a "variable" declaration - please consider putting this declaration in the article as well, or at least having a table with the corresponding declarations (esp. because e.g. sigma is used as a locally bound variable, while alpha is used as a datatype constructor, and only the color distinguishes these cases in the typesetting)

p.5: It would be nice if the _$_ notation were explained, or if ordinary parentheses were used. Not everyone who works with dependent types is a major Haskell user.

p6: The notation (Γ -Env) Var Δ was a source of confusion, initially. It seems to mix postfix and ordinary notation. Why can't Env just come first?

p6: Missing comma after "Thinnables" on the last line

p7: Fig. 2 is in a float, which means that the figure is left-justified. This looks very different from the Thinning definition earlier on the page, which is centered. There's a repeated issue in the article where there seems to be no pattern governing which code ends up in a figure and which ends up in a centered display box, and where the captions on figures have inconsistent font coloration and capitalization with respect to the main text.

p7: Fig 2 uses copatterns. These are introduced and cited much later in the article, but they're used here. Please move their introduction to the first place they're used. Copatterns are also used in both prefix notation and with the dotted projection notation - picking one and explaining it clearly would increase the accessibility of the paper to non-Agda-experts.

p8: The uses of script V and script C at the top of the page should be after their introductions halfway down the page.

p8, code: the code on page 8 seems to be mostly part of the record Semantics, but this took some reverse engineering. Can the fields be indented, as they would be in Agda? And are definitions like extend (e.g. manifest fields) allowed as part of a record? If so, say so, and make it clear which code is in the record body.

p8: When talking about the Kripke function space in NbE, please cite it so interested readers can follow up.

p8, last sentence: This sentence has a very complicated structure. What about a rewrite like "Agda allows us to package a generic traversal function "semantics" together with the fields of the record "Semantics", which causes it to be brought into scope for any instance of "Semantics".". And how is this different from having a field with a function type?

p8: This page has the first name that contains a caret (^). Please explain this naming convention - how should the caret be understood and pronounced? Is it intended to imply a superscript?

p9: At the beginning of 3.3, it would be nice to remind readers that, in the arguments to Semantics, the first argument is what gets substituted, and the second is the syntax being operated over. These sorts of little high-level descriptions would make the paper much less laborious to read.

p9: The sentence starting with "To print a variable" should be there to match the rest of the syntax, but it's not.

p9: "so it is automatically a Thinnable functor" - is this reflected in any code? Is this standing for an implementation of th^V?

p.10: The code in Fig 6 uses do-notation, but the code in Fig. 7 does not. The do-notation seems much easier to read. Can it be used in Fig. 7?

p.10: "Inductive-Recursive" should be lowercase

p.11: A high-level description of the roles played by I and J in Desc would be useful. Why are there two parameters?

p.11: "Indexes" should probably read "indices", esp. in an article using UK English

p.12: Later, on p. 15, the paper says that Agda uses pattern synonyms to resugar output. This discussion should perhaps occur here, at the first use of pattern synonyms.

p.16: "records explicitly the change" -> "explicitly records the change"

p.16: Here's another name with a ^ in it, still unexplained

p. 17: When something is called a lemma, it should have an English statement in addition to the encoding in Agda. This would make it much easier to figure out.

p. 18: There is a sentence "We immediately introduce a corollary ...", but it's not clear where that happens. This corollary should be in the text, rather than in a float on the next page, and it should also have an English statement that readers can compare to the Agda.

p. 19: Earlier uses of copatterns did them as prefix projections, Fig. 30 uses postfix.

p. 19: The sentence starting with "Hence, we can then implement..." trails off at a colon without ever being finished.

p. 20: The name M referred to here is defined on page 6. It would be best to give it a more memorable name, but failing that, it would be good to have an explicit page reference for its binding site so readers can go remind themselves of what it means.

p. 20: Couldn't parse the sentence "Wrapper it is Thinnable, and fresh (defined in Fig. 6) is the proof that we can generate placeholder values thanks to the name supply."

p.21: A sentence each describing mapA and sequenceA would be useful here.

p.22: "consist in type checking" -> "consists of type checking"

p. 23: The sentence "Terms in the raw syntax ..." was a bit hard to follow. Consider more commas, or breaking it in two.

p.23: I-dec could use a description/definition in the text

p.24: "bi-directional" shouldn't be hyphenated

p.25: The constructor name alpha didn't get mentioned before this page. Consider including a definition of Type somewhere.

p.25: "check the input type is" -> "check that the input type is"

p.25: The use of the term "cut" for the annotated embedding of checkable into inferrable terms is not widespread - consider explaining it here.

p.25: The alphas toward the bottom of the page should be in constructor font (green)

p.26: "During typechecking we generate at the same time an expression's type ..." -> "During typechecking we simultaneously generate an expression's type ..."

p.26: Couldn't parse "We can then explain what it means for elaboration to target T a (Type -Scoped) at a type sigma:". It looks like the variable T has type (Type -Scoped) in the code, but the rest confused me.

p.27: "do-notation" should be singular, not plural (do a search of the article)

p.27: The overloading in the definition of isArrow is confusing. Could one arrow be made different, perhaps the one in the view?

p. 28: What is bind again? The code b (bind Infer) (epsilon dot var_0) (sigma :: Gamma) tau  was pretty opaque.

p. 28: Consider the phrasing "Luckily, Agda's dependent do-notation makes our job easy once again:".

p.29: The patterns `IN and `IN' were difficult to distinguish at first

p.30: In figure 51, what is "pack"?

p.30: Hyphenate "size-preserving" and "size-respecting"

p.31: Redex is short for "reducible expression" - it's not a Latin word, so it should be pluralized "redexes".

p.32: "and computations are the pairing" --> "and whose computations are the pairing"

p.35: "much of the standard traversals" -> "many of the standard traversals" (because "traversal" is grammatically countable)

p.36: Copattern matching is introduced here, after it's been used numerous times already. Also, more extraneous parentheses in the citations.

p.37: "premisse" -> "premise"

p.38:  "generic" is repeated in the last line of the page. 

p.39: "and decision procedure can be defined" -> "and decision procedures can be defined"

p.39: The code in figure 72 is the first time the articule used lambda as an argument without either using $ or parentheses. When that's paired up with the where syntax, consider some extraneous parentheses.

p.40: What does it mean to have a module definition inside a record? Please explain this.

p.40: Break the sentence at "; which we have experience" and write "We experience this first hand when tackling..." as a new sentence.

p.41: "Results which" -> "Results that" (because it's a restrictive use)

p.42: "prevents to try to prove for instance that" occurs in a very long, run-on sentence. Consider reformulating the sentence to something like "Nothing prevents proofs, such as the idempotence of NbE, which use a bona fide reification function that extracts terms from model values"

p.43: The parenthesis opened after "on that term is related via" and before Kripke is never closed.

p.43: "This set of constraint" -> "This set of constraints"

p.43: The font in Fusion changes after the s, and it doesn't seem to be a reference to an identifier Fus in this article.

p.45: "the appropriate notion of Semantics" seems to have "Semantics" in the wrong font.

p. 46: "of each combinators" -> "of each combinator"

p. 46: The article should discuss how related work is related, not just list the fact that it is related. But the discussion of Back Poulsen et al doesn't discuss that relationship.

p.47: The vertical gap above 10.5 is very large.

p.47: Why are Pickering et al cited for Agda's pattern synonyms? Haskell's are much younger, and what's described in the cited paper.

p.47: "but also sum" -> "but also sums"

p.49: Under "closure under products" and "unrestricted variables", the word "kind" is used the way that "sort" was used in most of the article

p.50: "well-behaved" needs a hyphen
