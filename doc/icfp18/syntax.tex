
\section{Introduction}

In modern typed programming languages, programmers writing embedded
DSLs~\cite{hudak1996building} and researchers formalising them can now
use the host language's type system to help them. Using Generalised
Algebraic Data Types (GADTs) or the more general indexed families of
Type Theory~\cite{dybjer1994inductive} for representing their syntax,
programmers can \emph{statically} enforce some of the invariants in
their languages. Managing variable scope is a popular use
case~\cite{altenkirch1999monadic} as directly manipulating raw de
Bruijn indices is error-prone.  Solutions range from enforcing well
scopedness to ensuring full type and scope correctness. In short, we
use types to ensure that ``illegal states are unrepresentable'', where
illegal states are ill scoped or ill typed terms.

Despite the large body of knowledge in how to use types to define well
formed syntax (see the Related Work in Section
\ref{section:related-work}), it is still necessary for the working DSL
designer or formaliser to redefine essential functions like renaming
and substitution for each new syntax, and then to reprove essential
lemmas about those functions. To reduce the burden of such repeated
work and boilerplate, we apply the methodology of datatype-genericity to
programming and proving with syntaxes with binding.

To motivate our approach, let us look at the formalisation of an 
apparently straightforward program transformation: the inlining of
let-bound variables by substitution. You have two languages: the source
(\AD{S}), which has let-bindings, and the target (\AD{T}), which only
differs in that it does not:
\begin{displaymath}
  \AD{S} ::= x \mid \AD{S}~\AD{S} \mid \lambda x. \AD{S} \mid \textrm{let }x=\AD{S}\textrm{ in }\AD{S}
  \qquad
  \AD{T} ::= x \mid \AD{T}~\AD{T} \mid \lambda x. \AD{T}
\end{displaymath}

Breaking the task down, you need to define an operational semantics for
each language, define the program transformation itself, and prove a
correctness lemma that states each step in the source language is
simulated by zero or more steps of the transformed terms in the target
language. In the course of doing this, you discover that you actually
have a large amount of work to do:

\begin{enumerate}
\item To define the operational semantics you need to define
  substitution, and hence renaming, for both the source and target
  languages, even though they are very similar;
\item In the course of proving the correctness lemma, you discover
  that you need to prove eight lemmas about the interactions of
  renaming, substitution, and transformation that are all remarkably
  similar, but must be stated and proved separately (e.g, as in
  \cite{benton2012strongly}).
\end{enumerate}

Even after doing all of this work, you have only a result for a single
pair of source and target languages. If you were to change your
languages $\AD{S}$ or $\AD{T}$, you would have to repeat the same work
all over again (or at least do a lot of cutting, pasting, and
editing).
% The cases for $\lambda$s and applications in the
% transformation are structural, but if we add more constructs to both
% languages, we would have to write more and more code to say we are
% doing nothing interesting to them.

Using the universe of syntaxes with binding we present in this paper,
we are able to solve this repetition problem \emph{once and for all}.

%  For
% \emph{every} syntax in our universe, we are able to generically define
% renaming and substitution \ref{section:renandsub}, generically define
% the let-binding removal transformation \ref{section:letbinding}, and
% generically prove the required renaming, substitution and
% transformation fusion lemmas (\ref{section:fusion}.

\paragraph{Content and Contributions.}
We start with primers on scoped and sorted terms
(Section~\ref{section:primer-term}), scope and sort safe programs
acting on them (Section~\ref{section:primer-program}), and
programmable descriptions of data types (Section~\ref{section:data}).
These introductory sections help us build an understanding of the
problem at hand as well as a toolkit that leads us to the novel
content of this paper: a universe of scope safe syntaxes with binding
(Section~\ref{section:universe}) together with a notion of scope safe
semantics for these syntaxes (Section~\ref{section:semantics}).  This
gives us the opportunity to write generic implementations of renaming
and substitution (Section~\ref{section:renandsub}), a generic
let-binding removal transformation (generalising the problem stated
above) (Section~\ref{section:letbinding}), and normalisation by
evaluation (Section~\ref{section:nbyeval}). Further, we show how to
construct generic proofs by formally describing what it means for a
semantics to be able to simulate another one
(Section~\ref{section:simulation}), or for two semantics to be fusable
(Section~\ref{section:fusion}). This allows us to prove the lemmas
required above for renaming and substitution generically, for
\emph{every} syntax in our universe.

\medskip

Our implementation language is
Agda~\cite{norell2009dependently}. However, our techniques are
language independent: any dependently typed language at least as
powerful as Martin-L\"of Type Theory~\cite{martin1982constructive}
equipped with inductive families~\cite{dybjer1994inductive} such as
Coq~\cite{Coq:manual}, Lean~\cite{DBLP:conf/cade/MouraKADR15} or
Idris~\cite{brady2013idris} ought to do.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SCOPE SAFE TERMS


\section{A Primer on Scope And Sort Safe Terms}\label{section:primer-term}

Scope safe terms follow the discipline that every variable is
either bound by some binder or is explicitly accounted for in a
context. Bellegarde and Hook~\citeyear{BELLEGARDE1994287}, Bird and Patterson~\citeyear{bird_paterson_1999},
and Altenkirch and Reus~\citeyear{altenkirch1999monadic} introduced the
classic presentation of scope safety using inductive
\emph{families}~\cite{dybjer1994inductive} instead of inductive types to
represent abstract syntax. Indeed, using a family indexed by a $\Set{}$,
we can track scoping information at the type level. The empty $\Set$ represents the empty scope. The functor
$1 + (\_)$ extends the running scope with an extra variable.

An inductive type is the fixpoint of an endofunctor on $\Set{}$.
Similarly, an inductive family is the fixpoint of an endofunctor on
$\Set \to \Set$. Using inductive families to enforce scope safety, we
get the following definition of the untyped $\lambda$-calculus: $T(F)
= \lambda X \!\in\! \Set{}.\; X + (F(X) \times F(X)) + F(1 + X)$.
This endofunctor offers a choice of three constructors.  The first one
corresponds to the variable case; it packages an inhabitant of $X$,
the index $\Set{}$. The second corresponds to an application node;
both the function and its argument live in the same scope as the
overall expression. The third corresponds to a $\lambda$-abstraction;
it extends the current scope with a fresh variable.  The language is
obtained as the fixpoint of $T$:
\[
   \mathit{Lam} = \mu F \in \Set{}^{\Set{}}.
   \lambda X \!\in\! \Set{}.\; X + (F(X) \times F(X)) + F(1 + X)
\]
Since `Lam' is a endofunction on Set, it makes
sense to ask whether it is also a functor and a monad. Indeed it is,
as Altenkirch and Reus have shown. The functorial action corresponds
to renaming, the monadic `return' corresponds to the use of variables,
and the monadic `join' corresponds to substitution. The functor and
monad laws correspond to well known properties from the equational
theories of renaming and substitution. We will revisit these properties
below in Section~\ref{section:fusion}.

\paragraph{A Mechanized Typed Variant of Altenkirch and Reus' Calculus.}\label{section:mech-reus}

There is no reason to restrict this technique to fixpoints of endofunctors
on $\Set{}^{\Set{}}$. The more general
case of fixpoints of (strictly positive) endofunctors on $\Set{}^J$ can be
endowed with similar operations by using Altenkirch, Chapman and
Uustalu's relative monads~\citeyear{Altenkirch2010, JFR4389}.

We pick as our $J$ the category whose objects are inhabitants of
\AD{List} \AB{I} (\AB{I} is a parameter of the construction) and whose morphisms are
thinnings (see Section~\ref{def:thinning}).  This \AD{List} \AB{I} is
intended to represent the list of the sort (/ kind / types depending
on the application) of the de Bruijn variables in scope. We can
recover an untyped approach by picking $I$ to be the unit type.  Given
this typed setting, our functors take an extra $I$ argument
corresponding to the type of the expression being built. This is
summed up by the large type \AB{I}
\AF{─Scoped}:% = $I$ $\to$ \AD{List} $I$ $\to$ \AF{Set}.

\begin{center}
\ExecuteMetaData[var.tex]{scoped}
%\caption{Type of Well \AB{I}-Kinded and Well Scoped Families}
\end{center}

We use Agda's mixfix operator notation where underscores denote
argument positions.

To lighten the presentation, we exploit the observation that the
current scope is either passed unchanged to subterms (e.g. in the application case)
or extended (e.g. in the $\lambda$-abstraction case) by introducing combinators
to build indexed types.
% All the definitions are polymorphic in the index type
% (called \AB{A} in Figure~\ref{figure:indexed}).

\begin{center}
\begin{minipage}{0.4\textwidth}
\ExecuteMetaData[indexed.tex]{arrow}
\end{minipage}\hfill
\begin{minipage}{0.5\textwidth}
\ExecuteMetaData[indexed.tex]{adjust}
\end{minipage}

\begin{minipage}{0.40\textwidth}
\ExecuteMetaData[indexed.tex]{product}
\end{minipage}\hfill
\begin{minipage}{0.25\textwidth}
\ExecuteMetaData[indexed.tex]{constant}
\end{minipage}\hfill
\begin{minipage}{0.25\textwidth}
\ExecuteMetaData[indexed.tex]{forall}
\end{minipage}
%\caption{Combinators to build indexed Sets}\label{figure:indexed}
\end{center}

We lift the function space and the product type pointwise with
\AF{\_⟶\_} and \AF{\_∙×\_} respectively, silently threading the
underlying scope. The \AF{\_⊢\_} makes explicit the \emph{adjustment} made to the index
by a function, conforming to the convention (see e.g.~\cite{martin1982constructive}) of mentioning only
context \emph{extensions} when presenting judgements 
and write \AB{f} \AF{⊢} \AB{T} where \AB{f} is the modification and \AB{T} the indexed
Set it operates on. Although it may seem surprising at first to define binary
infix operators as having arity three, they are meant to be used partially applied,
surrounded by \AF{[\_]} which turns an indexed Set into a Set by implicitly
quantifying over the index.
Lastly, \AF{κ} is the constant combinator, ignoring the index.

We make \AF{⟶} associate to the right as one would expect and give it the
highest precedence level as it is the most used combinator. These combinators
lead to more readable type declarations.  For instance, the compact expression
\AF{[} \AF{suc} \AF{⊢} (\AB{P} \AF{∙×} \AB{Q}) \AF{⟶} \AB{R} \AF{]}
desugars to the more verbose type
\AS{∀} \{\AB{i}\} \AS{→} (\AB{P} (\AF{suc} \AB{i}) \AF{×} \AB{Q} (\AF{suc} \AB{i})) \AS{→} \AB{R} \AB{i}.

As the context comes second in the definition of \AF{\_─Scoped}, we
can readily use these combinators to thread, modify, or quantify over the
scope when defining such families:

%\begin{center}
\begin{minipage}[t]{0.35\textwidth}
\ExecuteMetaData[var.tex]{var}
\end{minipage}\hfill
\begin{minipage}[t]{0.55\textwidth}
\ExecuteMetaData[motivation.tex]{tm}
\end{minipage}
%\caption{Scope Aware Variables and Simply Typed $\lambda$-Terms\label{scoped-untyped}}
%\end{center}

The inductive family \AD{Var} represents well scoped and well kinded
de Bruijn~\citeyear{de1972lambda}
indices. Its \AIC{z} (for zero) constructor refers to
the nearest binder in a non-empty scope. The \AIC{s} (for successor) constructor lifts a
a variable in a given scope to the extended scope where
an extra variable has been bound. Both of the constructors' types have been written using the combinators defined above.
They respectively normalise to:
\begin{center}
  \AIC{z} : {\AS{∀} \AB{i} \AB{xs} \AS{→} \AD{Var} \AB{i} (\AB{i} \AIC{:\!:} \AB{xs})}
  \qquad
  \AIC{s} : {\AS{∀} \AB{i} \AB{j} \AB{xs} → \AD{Var} \AB{i} \AB{xs} \AS{→} \AD{Var} \AB{i} (\AB{j} \AIC{:\!:} \AB{xs})}
\end{center}

The \AD{Type} \AF{─Scoped} family \AD{Lam} is Altenkirch
and Reus' simply typed $\lambda$-calculus representation. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SCOPE SAFE PROGRAMS


\section{A Primer on Type and Scope Safe Programs}\label{section:primer-program}

The scope-and-type safe representation described in the previous section is
naturally only a start: once the programmer has access to a good
representation of the language they are interested in, they will naturally
want to (re)implement standard traversals manipulating terms.
Renaming and substitution are the two most typical examples
of such traversals. Now that well-typedness and well-scopedness are enforced
statically, all of these traversals have to be implemented
in a type and scope safe manner.
These constraints show up in the types of renaming and substitution: % defined as follows:

\begin{figure}[h]
\begin{minipage}{0.50\textwidth}
\ExecuteMetaData[motivation.tex]{ren}
\end{minipage}\hfill
\begin{minipage}{0.50\textwidth}
\ExecuteMetaData[motivation.tex]{sub}
\end{minipage}
\caption{Type and Scope Preserving Renaming and Substitution}
\end{figure}

We have voluntarily hidden technical details behind some auxiliary definitions
left abstract here: \AF{⟦V⟧} and \AF{extend}. Their implementations are distinct
for \AF{ren} and \AF{sub} but they serve the same purpose: \AF{⟦V⟧} is used to
turn a value looked up in the evaluation environment into a term and \AF{extend}
is used to alter the environment when going under a binder. This presentation
highlights the common structure between \AF{ren} and \AF{sub} which we will exploit
later in this section, particularly in Figures~\ref{figure:lamsem} and \ref{figure:fdmlamsem}
where we define an abstract notion of semantics and the corresponding generic traversal.

Both renaming and substitution are defined in terms of \emph{environments}
{(\AB{Γ} \AR{─Env}) \AB{𝓥} \AB{Δ}} that describe how to associate a value \AB{𝓥}
(variables for renaming, terms for substitution) well scoped and typed in \AB{Δ}
to every entry in \AB{Γ}. Environments are defined as the following record
structure (using a record helps Agda's type inference reconstruct the type of
values \AB{𝓥} for us):

%\begin{figure}[h]
\begin{center}
\ExecuteMetaData[environment.tex]{env}
\end{center}
%\caption{Well Typed and Scoped Environments of Values}
%\end{figure}

As we have already observed, the definitions of renaming and substitution have very
similar structure. Abstracting away this shared structure would allow for these
definitions to be refactored, and their common properties to be proved in one swift
move.

Previous efforts in dependently typed
programming~\cite{benton2012strongly,allais2017type}
have achieved this goal and refactored renaming and substitution,
but also normalisation by evaluation, printing with names or CPS conversion
as various instances of a more general traversal. As we will show in Section~\ref{section:typechecking},
typechecking in the style of Atkey~\citeyear{atkey2015algebraic} also
fits in that framework. To make sense of this body of work, we
need to introduce three new notions: \AF{Thinning}, a generalisation of
renaming; \AF{Thinnable}s which are types that permit thinning; and the
\AF{□} functor, which freely adds Thinnability to any indexed type.
%%%
We use \AF{□}, and our compact notation for the indexed function space
between indexed types, to crisply encapsulate the additional quantification 
over environment extensions which is typical of Kripke semantics.
%%%
\label{def:thinning}
%\begin{figure}[h]
\begin{center}
\ExecuteMetaData[environment.tex]{thinning}
\end{center}
%\caption{Thinnings: A Special Case of Environments}
%\end{figure}

\AF{Thinning}s subsume more structured notions such as the Category of
Weakenings~\cite{altenkirch1995categorical} or Order Preserving
Embeddings~\cite{chapman2009type}. In particular, they do not prevent the
user from defining arbitrary permutations or from introducing contractions
although we will not use such instances. However, such extra flexibility
will not get in our way, and permits a representation as a function space
which grants us monoid laws ``for free'' as per Jeffrey's
observation~\citeyear{jeffrey2011assoc}.

The \AF{□} combinator turns any (\AD{List} \AB{I})-indexed Set into one that can
absorb thinnings. This is accomplished by abstracting over all possible thinnings
from the current scope, akin to an S4-style necessity modality. The axioms of S4
modal logic incite us to observe that the functor \AF{□} is a comonad: \AF{extract}
applies the identity \AF{Thinning} to its argument, and \AF{duplicate} is obtained
by composing the two \AF{Thinning}s we are given. The expected laws hold trivially
thanks to Jeffrey's trick mentioned above.

The notion of \AF{Thinnable} is the property of being stable under thinnings;
in other words \AF{Thinnable}s are the coalgebras of \AF{□}.
It is a crucial property for values to have if one wants to be able to push
them under binders. From the comonadic structure we get that
the \AF{□} combinator freely turns any (\AD{List} I)-indexed Set into a
\AF{Thinnable} one.

\begin{figure}[h]
\begin{minipage}{0.45\textwidth}
\ExecuteMetaData[environment.tex]{box}
\ExecuteMetaData[environment.tex]{comonad}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\ExecuteMetaData[environment.tex]{thinnable}
\ExecuteMetaData[environment.tex]{freeth}
\end{minipage}
\caption{The \AF{□} comonad, Thinnable, and the cofree Thinnable.}
\end{figure}

As Allais, Chapman, McBride and McKinna (ACMM)~\citeyear{allais2017type} shows,
equipped with these new notions
we can define an abstract concept of semantics for our scope-and-type safe language
(cf. Figures~\ref{figure:lamsem} and \ref{figure:fdmlamsem}).
Broadly speaking, a semantics turns our deeply embedded abstract
syntax trees into the shallow embedding of the corresponding
parametrised higher order abstract syntax term. We get a choice of
useful scope-and-sort safe traversals by using different `host languages'
for this shallow embedding.

Semantics, 
   specified in terms of a record \AR{Sem}, 
are defined in Figure~\ref{figure:lamsem} in terms of a choice of values \AB{$\mathcal{V}$} and
computations \AB{$\mathcal{C}$}. Realisation of a semantics will produce a
computation in \AB{$\mathcal{C}$} for every term whose variables are assigned
values in \AB{$\mathcal{V}$} as demonstrated in Figure~\ref{figure:fdmlamsem}.
A semantics must satisfy constraints on the notions of
values \AB{$\mathcal{V}$} and computations \AB{$\mathcal{C}$} at hand. First of all,
values should be thinnable so that \AF{sem} may push the environment
under binders. Second, the set of computations needs to be closed
under various combinators which are the semantical counterparts of
the language's constructors. The semantical counterpart of application is an operation
that takes a representation of a function and a representation of an
argument and produces a representation of the result.
The interpretation of the $\lambda$-abstraction is of particular interest:
it is a variant on
the Kripke function space one can find in normalisation by evaluation.
In all possible thinnings of the scope at hand, it promises to deliver
a computation whenever it is provided with a value for its newly
bound variable. This is concisely expressed by the type
(\AF{□} (\AB{$\mathcal{V}$} \AB{$\sigma$} \AF{→} \AB{$\mathcal{C}$} \AB{$\tau$})).

\begin{figure}[h]
\ExecuteMetaData[motivation.tex]{rsem}
\caption{Semantics for \AD{Lam}}\label{figure:lamsem}
\end{figure}

Agda allows us to package, together with the fields of the record
\AR{Sem}, the generic traversal function \AF{sem}, which is brought
into scope for any instance of \AR{Sem}. We thus realise the promise
made earlier, namely that any given {\AR{Sem} \AB{$\mathcal{V}$}
  \AB{$\mathcal{C}$}} induces a function which, given a value in
\AB{$\mathcal{V}$} for each variable in scope, transforms a  \AD{Lam} term into a
computation \AB{$\mathcal{C}$}.

\begin{figure}[h]
\ExecuteMetaData[motivation.tex]{sem}
\caption{Fundamental Lemma of Semantics for \AD{Lam}, relative to a given \AR{Sem}\;\AB{$\mathcal{V}$}\;\AB{$\mathcal{C}$}}\label{figure:fdmlamsem}
\end{figure}

Coming back to renaming and substitution, we see that they both fit in the
\AR{Sem} framework. We notice that the definition of substitution depends on
the definition of renaming: to be able to push terms under binder, we need to
have already proven that they are thinnable.

\begin{figure}[h]
\begin{minipage}{0.5\textwidth}
\ExecuteMetaData[motivation.tex]{semren}
\ExecuteMetaData[motivation.tex]{semrenfun}
\end{minipage}\hfill
\begin{minipage}{0.5\textwidth}
\ExecuteMetaData[motivation.tex]{semsub}
\ExecuteMetaData[motivation.tex]{semsubfun}
\end{minipage}
\caption{Renaming and Substitution as Instances of \AR{Sem}}
\end{figure}

In both cases we use (\AIC{pack} \AIC{s}) (where \AIC{pack} is the constructor
for environments and \AIC{s}, defined in Section \ref{section:mech-reus}, is the function
lifting an existing de Bruijn variable
into an extended scope) as the definition of the thinning embedding \AB{$\Gamma$}
into {\AB{$\sigma$} \AIC{::} \AB{$\Gamma$}}.

We also include the definition of a basic printer relying on a
name supply to highlight the fact that computations can very well be
effectful. The \AF{Printing} semantics is defined by using \AD{String}s
as values and {\AD{State} \AD{$\mathbb{N}$} \AD{String}} as computations. We use a
\AR{Wrap}per with a type and a context as phantom types in order to
help Agda's inference propagate the appropriate constraints.
We define a function \AF{fresh} that generates new concrete names using a
\AD{State} monad.

\begin{figure}[h]
  \ExecuteMetaData[motivation.tex]{valprint}
  \ExecuteMetaData[motivation.tex]{freshprint}
\caption{Wrapper and fresh name generation}
\end{figure}

The wrapper \AR{Wrap} does not depend on the scope \AB{$\Gamma$} so it is automatically
a Thinnable functor. We jump straight
to the definition of the printer. To print an application, we produce
a string representation of the term in function position, then of its argument and combine
them by putting the argument between parentheses. To print a $\lambda$-abstraction,
we start by generating a fresh name for the newly-bound variable, use that
name to generate a string representing the body of the function to which we
prepend a ``$\lambda$'' binding the fresh name.

\begin{figure}[h]
\ExecuteMetaData[motivation.tex]{semprint}
\caption{Printing as an instance of \AR{Sem}}
\end{figure}

Both printing and renaming highlight the importance of distinguishing
values and computations: the type of values in their respective
environments are distinct from their type of computations.

All of these examples are already described at length by ACMM~\citeyear{allais2017type}
so we will not spend any
more time on them. They have also obtained the simulation and fusion
theorems demonstrating that these traversals are well behaved as
corollaries of more general results expressed in terms of \AF{sem}.
We will come back to this in Section~\ref{section:simulation}.

One important observation to make is the tight connection between the constraints
described in \AR{Sem} and the definition of \AD{Lam}: the semantical counterparts
of the \AD{Lam} constructors are obtained by replacing the recursive occurences of
the inductive family with either a computation or a Kripke function space depending
on whether an extra variable was bound. This suggests that it ought to be possible
to compute the definition of \AF{Sem} from the syntax description. Before doing this
in Section~\ref{section:universe}, we need to look at a generic descriptions of
datatypes.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% DATATYPES

\section{A Primer on the Universe of Data Types}\label{section:data}

Chapman, Dagand, McBride and Morris (CDMM)~\citeyear{Chapman:2010:GAL:1863543.1863547}
defined a universe of data types inspired by Dybjer and Setzer's
finite axiomatisation of Inductive-Recursive definitions~\citeyear{Dybjer1999}
and Benke, Dybjer and Jansson's universes for generic programs and proofs~\citeyear{benke-ugpp}.
This explicit definition of \emph{codes} for data types empowers the
user to write generic programs tackling \emph{all} of the data types
one can obtain this way. In this section we recall the main aspects
of this construction we are interested in to build up our generic
representation of syntaxes with binding.

The first component of CDMM's universe's definition is an inductive type
of \AD{Desc}riptions of strictly positive functors from $\Set{}^J$ to
$\Set{}^I$. It has three constructors: \AIC{`σ} to store data (the rest of
the description can depend upon this stored value), \AIC{`X} to attach a
recursive substructure indexed by $J$ and \AIC{`$\blacksquare$} to stop with a particular
index value.

The recursive function \AF{⟦\_⟧} makes the interpretation of the
descriptions formal. Interpretation of descriptions give rise right-nested tuples
terminated by equality constraints.

\begin{figure}[h]
\begin{minipage}{0.45\textwidth}
\ExecuteMetaData[Generic/Data.tex]{desc}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\ExecuteMetaData[Generic/Data.tex]{interp}
\end{minipage}
\caption{Datatype Descriptions and their Meaning as Functors}\label{figure:desc}
\end{figure}

These constructors give the programmer the ability to build up the data
types they are used to. For instance, the functor corresponding
to lists of elements in $A$ stores a \AD{Bool}ean which stands for whether
the current node is the empty list or not. Depending on its value, the
rest of the description is either the ``stop'' token or a pair of an element
in $A$ and a recursive substructure i.e. the tail of the list. The \AD{List} type
is unindexed, we represent the lack of an index with the unit type \AD{$\top$}.

\begin{figure}[h]
\ExecuteMetaData[Generic/Data.tex]{listD}
\caption{The Description of the base functor for \AD{List} \AB{A}}\label{figure:listD}
\end{figure}

Indexes can be used to enforce invariants. For example, the type {\AD{Vec} \AB{A} \AB{n}}
of length-indexed lists. It has the same structure as the definition of \AF{listD}.
We start with a \AF{Bool}ean distinguishing the two constructors: either
the empty list (in which case the branch's index is enforced to be $0$) or a
non-empty one in which case we store a natural number \AB{n}, the head of type
\AB{A} and a tail of size \AB{n} (and the branch's index is enforced to be
\AIC{suc} \AB{n}).

\begin{figure}[h]
\ExecuteMetaData[Generic/Data.tex]{vecD}
\caption{The Description of the base functor for \AD{Vec} \AB{A} \AB{n}}\label{figure:vecD}
\end{figure}

The payoff for encoding our datatypes as descriptions is that we can define
generic programs for whole classes of data types. The decoding function \AF{⟦\_⟧}
acted on the objects of $\Set{}^J$, and we will now define the function \AF{fmap} by
recursion over a code \AB{d}. It describes the action of the functor corresponding
to \AB{d} over morphisms in $\Set{}^J$. This is the first example of generic
programming over all the functors one can obtain as the meaning of a description.

\begin{figure}[h]
\ExecuteMetaData[Generic/Data.tex]{fmap}
\caption{Action on Morphisms of the Functor corresponding to a \AF{Desc}ription}
\end{figure}

All the functors obtained as meanings of \AD{Desc}riptions are strictly
positive. So we can build the least fixpoint of the ones that are endofunctors
(i.e. the ones for which $I$ equals $J$). This fixpoint is called \AD{μ}
and its iterator is given by the definition of \AF{fold} \AB{d}%%%
\footnote{\textbf{NB} 
In Figure~\ref{figure:datamu} the \AD{Size}~\cite{DBLP:journals/corr/abs-1012-4896} index added
to the inductive definition of \AD{μ} plays a crucial role in getting the
termination checker to see that \AF{fold} is a total function. 
% Sized types are outside the scope of this paper; describing them in
% detail would be a distraction.  
}
.

\begin{figure}[h]
\ExecuteMetaData[Generic/Data.tex]{mu}
\ExecuteMetaData[Generic/Data.tex]{fold}
\caption{Least Fixpoint of an Endofunctor and Corresponding Generic Fold}\label{figure:datamu}
\end{figure}


The CDMM approach therefore allows us to generically define iteration principles
for all data types that can be described. These are exactly the features we desire
for a universe of data types with binding, so in the next section we will see how
to extend CDMM's approach to include binding.

The functor underlying any well scoped and sorted syntax can be coded as some
{\AD{Desc} (I \AR{$\times$} \AD{List} I) (I \AR{$\times$} \AD{List} I)}, with the
free monad construction from CDMM uniformly adding the variable case. Whilst a
good start, \AD{Desc} treats its index types as unstructured, so this construction
is blind to what makes the {\AD{List} I} index a \emph{scope}. The resulting
`bind' operator demands a function which maps variables in \emph{any} sort and
scope to terms in the \emph{same} sort and scope. However, the behaviour we need
is to preserve sort while mapping between specific source and target scopes which
may differ. We need to account for the fact that scopes change only by extension,
and hence that our specifically scoped operations can be pushed under binders by
weakening.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% UNIVERSE OF SYNTAXES


\section{A Universe of Scope Safe and Well Kinded Syntaxes}\label{section:universe}

Our universe of scope safe and well kinded syntaxes follows the same principle
as CDMM's universe of datatypes, except that we are not building endofunctors on
\AS{Set} any more but rather on {\AB{I} \AF{─Scoped}}. We now think of the index
type \AB{I} as the sorts
used to distinguish terms in our embedded language. The \AIC{`$\sigma$} and
\AIC{`∎} constructors are as in the CDMM \AD{Desc} type, and are used to
represent data and index constraints respectively. 
%%%
What distinguishes this new universe \AD{Desc} from that of Section~\ref{section:data} 
is that the
%%%     The 
\AIC{`X} constructor
is now augmented with an additional {\AD{List} \AB{I}} argument that describes
the new binders that are brought into scope at this recursive position. This
list of the kinds of the newly-bound variables will play a crucial role when
defining the description's semantics as a binding structure in
Figures~\ref{figure:syntaxmeaning}, \ref{figure:debruijnscope} and \ref{figure:freemonad}.

\begin{figure}[h]
\ExecuteMetaData[Generic/Syntax.tex]{desc}
\caption{Syntax Descriptions}
\end{figure}

The meaning function \AF{⟦\_⟧} we associate to a description follows closely
its CDMM equivalent. It only departs from it in the \AIC{`X} case and the fact
it is not an endofunctor on \AB{I} \AF{─Scoped}; it is more general than that.
The function takes an \AB{X} of type {\AD{List} \AB{I} $\rightarrow$ \AB{I} \AD{─Scoped}}
to interpret {\AIC{`X} \AB{Δ} \AB{j}} (i.e. substructures of sort \AB{j} with
newly-bound variables in \AB{Δ}) in an ambient scope \AB{Γ} as {\AB{X} \AB{Δ} \AB{j} \AB{Γ}}.

\begin{figure}[h]
\ExecuteMetaData[Generic/Syntax.tex]{interp}
\caption{Descriptions' Meanings}\label{figure:syntaxmeaning}
\end{figure}

The astute reader may have noticed that \AF{⟦\_⟧} is uniform in $X$ and $\Gamma$; however
refactoring \AF{⟦\_⟧} to use the partially applied $X\,\_\,\_\,\Gamma$ following
this observation would lead to a definition harder to use with the
combinators for indexed sets described in Section \ref{section:mech-reus}
which make our types much more readable.

If we pre-compose the meaning function \AF{⟦\_⟧} with a notion of `de Bruijn scopes'
(denoted \AF{Scope} here) which turns any \AB{I} \AF{─Scoped} family into a function
of type \AD{List} \AB{I} \AS{→} \AB{I} \AF{─Scoped} by appending the two
\AD{List} indices, we recover a meaning function producing an endofunctor on
\AB{I} \AF{─Scoped}. So far we have only shown the action of the functor on objects;
its action on morphisms is given by a function \AF{fmap} defined by induction over
the description just like in Section~\ref{section:data}.

\begin{figure}[h]
\ExecuteMetaData[Generic/Syntax.tex]{scope}
\caption{De Bruijn Scopes}\label{figure:debruijnscope}
\end{figure}

The endofunctors thus defined are strictly positive and we can take their fixpoints.
As we want to define the terms of a language with variables, instead of
considering the initial algebra, this time we opt for the free relative
monad~\cite{JFR4389} (with respect to the functor \AF{Var}): the \AIC{`var}
constructor corresponds to return, and we will define bind (also known as
the parallel substitution \AF{sub}) in the next section. 

% We have once more a \AD{Size} index to get all the benefits of type
% based termination checking.

\begin{figure}[h]
\ExecuteMetaData[Generic/Syntax.tex]{mu}
\caption{Term Trees: The Free \AF{Var}-Relative Monads on Descriptions}\label{figure:freemonad}
\end{figure}

Coming back to our original examples, we now have the ability to give
codes for the well scoped untyped $\lambda$-calculus and, just as well,
the intrinsically typed simply typed $\lambda$-calculus.
The variable case will be added by the free monad construction so we
only have to describe two constructors: application where we have two
substructures which do not bind any extra argument and $\lambda$-abstraction
which has exactly one substructure with precisely one extra bound variable.
In the untyped case a single \AD{Bool}ean is enough to distinguish the two constructors
whilst in the typed case, we need our tags to carry extra information about the
%%% JHM version
types involved so we use the ad-hoc \AD{`STLC} type, 
and its decoding \AD{STLC} defined by a pattern-matching \(\lambda\)-expression in Agda.
%%% GALLAIS version
% types involved so we use the ad-hoc \AD{`STLC} type. In the definition of
% \AF{STLC}, ``{\AS{$\lambda$} \AK{where}}'' introduces a pattern-matching lambda
% which allows us to immediately inspect the tag bound in the \AIC{`$\sigma$} node.

\begin{figure}[h]
\begin{minipage}{0.4\textwidth}
  \ExecuteMetaData[Generic/Examples/UntypedLC.tex]{ULC}
\end{minipage}\hfill
\begin{minipage}{0.5\textwidth}
  \ExecuteMetaData[Generic/Examples/STLC.tex]{stlc}
\end{minipage}
\caption{Examples: The Untyped and Simply Typed Lambda Calculi}
\end{figure}

For convenience we use Agda's pattern synonyms corresponding to the
original constructors in Section \ref{section:mech-reus}: \AIC{`V} for
\AIC{V} the variable constructor, \AIC{`A} for \AIC{A} the application
one and \AIC{`L} for \AIC{L} the $\lambda$-abstraction.  These
synonyms can be used when pattern-matching on a term and Agda resugars
them when displaying a goal. This means that the end user can
seamlessly work with encoded terms without dealing with the gnarly
details of the encoding.  These pattern definitions can omit some
arguments by using ``\AS{\_}'', in which case they will be filled in
by unification just like any other implicit argument: there is no
extra cost to using an encoding!  The only downside is that the
language currently does not allow the user to specify type annotations
for pattern synonyms.

\begin{figure}[h]
\begin{minipage}{0.40\textwidth}
 \ExecuteMetaData[Generic/Examples/UntypedLC.tex]{LCpat}
\end{minipage}\hspace{2em}
\begin{minipage}{0.50\textwidth}
\ExecuteMetaData[Generic/Examples/STLC.tex]{patST}
\end{minipage}
\caption{Respective Pattern Synonyms for the Untyped and Simply Typed Lambda Calculus}
\end{figure}

It is the third time (the first and second times being the definition of
\AF{listD} and \AF{vecD} in Figure~\ref{figure:listD} and \ref{figure:vecD})
that we use a \AF{Bool} to distinguish between two constructors. In order
to avoid re-encoding the same logic,
the next section introduces combinators demonstrating that
descriptions are closed under finite sums and finite products
of recursive positions.

\paragraph{Common Combinators and Their Properties.}\label{desccomb}

As seen previously, we can use a dependent pair whose first component
is a \AF{Bool}ean to take the coproduct of two descriptions: depending
on the value of the first component, we will return one or the other.
We can abstract this common pattern as a combinator \AF{\_`+\_} together
with an appropriate eliminator \AF{case} which, given two continuations,
picks the one corresponding to the chosen branch.

\begin{figure}[h]
\begin{minipage}{0.45\textwidth}
  \ExecuteMetaData[Generic/Syntax.tex]{sumcomb}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
  \ExecuteMetaData[Generic/Syntax.tex]{case}
\end{minipage}
\caption{Descriptions are closed under Sum}\label{figure:descsum}
\end{figure}

Closure under product does not hold in general. Indeed, the
equality constraints introduced by the two end tokens of two
descriptions may be incompatible. So far, a limited form of
closure (closure under finite product of recursive positions)
has been sufficient for all of our use cases. As with coproducts,
the appropriate eliminator \AF{unXs} takes a value in the encoding
and extracts its constituents ({\AF{All} \AB{P} \AB{xs}} is defined
in Agda's standard library and makes sure that the predicate \AB{P}
holds true of all the elements in the list \AB{xs}).

\begin{figure}[h]
\begin{minipage}{0.4\textwidth}
  \ExecuteMetaData[Generic/Syntax.tex]{paircomb}
\end{minipage}\hfill
\begin{minipage}{0.5\textwidth}
  \ExecuteMetaData[Generic/Syntax.tex]{pairunpair}
\end{minipage}
\caption{Descriptions are closed under Finite Products of Recursive Positions}
\end{figure}

A concrete use case for both of these combinators will be given in section~\ref{section:letbinding}
where we explain how to seamlessly enrich an existing syntax with let-bindings
and how to use the \AR{Sem} framework to elaborate them away.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% GENERIC SEMANTICS


\section{Generic Scope Safe and Well Kinded Programs for Syntaxes}\label{section:semantics}

Based on the \AR{Sem} type we defined for the specific example of the
simply typed $\lambda$-calculus in Section~\ref{section:primer-program},
we can define a generic notion of
semantics for all syntax descriptions. It is once more parametrised
by two \AB{I}\AF{─Scoped} families \AB{𝓥} and \AB{𝓒} corresponding
respectively to values associated to bound variables and
computations delivered by evaluating terms. These two families
have to abide by three constraints:
\begin{itemize}
\item{\ARF{th$^{\mathcal{V}}$}} Values should be thinnable so that we can push the
      evaluation environment under binders;
\item{\ARF{var}} Values should embed into computations for us to be able
      to return the value associated to a variable as the
      result of its evaluation;
\item{\ARF{alg}} We should have an algebra turning
      a term whose substructures have been replaced with
      computations (possibly under some binders, represented semantically
      by the \AF{Kripke} type-valued function defined below) into computations
\end{itemize}

\begin{figure}[h]
\ExecuteMetaData[Generic/Semantics.tex]{semantics}
\caption{A Generic Notion of Semantics}
\end{figure}

Here we crucially use the fact that the meaning of a description is
defined in terms of a function interpreting substructures which has
the type \AF{List} \AB{I} \AS{→} \AB{I}\AF{─Scoped}, i.e. that gets access
to the current scope but also the exact list of the newly bound variables' kinds.
We define a function \AF{Kripke} by case analysis on the number of newly bound
variables. It is essentially a subcomputation waiting for a value associated to
each one of the fresh variables.
\begin{itemize}
\item If it's $0$ we expect the substructure to be a computation corresponding
    to the result of the evaluation function's recursive call;
  \item But if there are newly bound variables then we expect to have a function
    space. In any context extension, it will take an environment of values for
    the newly-bound variables and produce a computation corresponding to the
    evaluation of the body of the binder.
\end{itemize}

\begin{figure}[h]
\ExecuteMetaData[environment.tex]{kripke}
\caption{Substructures as either Computations or Kripke Function Spaces}
\end{figure}

It is once more the case that the abstract notion of Semantics comes
with a fundamental lemma: all \AB{I} \AF{─Scoped} families \AB{𝓥} and
\AB{𝓒} satisfying the three criteria we have put forward give rise
to an evaluation function. We introduce a notion of computation
\AF{\_─Comp} analogous to that of environments: instead of associating
values to variables, it associates computations to terms.

%\begin{figure}[h]
\begin{center}
  \ExecuteMetaData[Generic/Semantics.tex]{comp}
\end{center}
%\caption{\AF{\_─Comp}: Associating Computations to Terms}
%\end{figure}

\subsection{Fundamental Lemma of Semantics}

We can now define the type of the fundamental lemma (called \AF{sem}) which
takes a semantics and returns a function from environments to computations.
It is defined mutually with a
function \AF{body} turning syntactic binders into semantics binders: to
each de Bruijn \AF{Scope} (i.e. a substructure in a potentially extended
context) it associates a \AF{Kripke} (i.e. a subcomputation expecting a
value for each newly bound variable).

\begin{figure}[h]
\ExecuteMetaData[Generic/Semantics.tex]{semtype}
\caption{Statement of the Fundamental Lemma of \AR{Sem}antics}
\end{figure}

The proof of \AF{sem} is straightforward now that we have clearly
identified the problem structure and the constraints we need to enforce.
We use postfix projections (of the form \AS{.}\ARF{name}) to make use of
the semantic combinators packaged in the \AR{Sem} parameter \AB{$\mathcal{S}$}.
If the term considered is a variable, we lookup the associated value in
the evaluation environment and turn it into a computation using \ARF{var}.
If it is a non variable constructor then we call \AF{fmap} to evaluate the
substructures using \AF{body} and then call the \ARF{alg}ebra to combine
these results.

\begin{figure}[h]
\ExecuteMetaData[Generic/Semantics.tex]{sem}
\caption{Proof of the Fundamental Lemma of \AR{Sem}antics -- \AF{sem}}
\end{figure}

The auxiliary lemma \AF{body} distinguishes two cases. If no new
variable has been bound in the recursive substructure, it is
a matter of calling \AF{sem} recursively. Otherwise we are provided
with a \AF{Thinning}, some additional values and evaluate the
substructure in the thinned and extended evaluation environment
(thanks to a auxiliary function \AF{\_>>\_} which given two environments
{(\AB{Γ} \AR{─Env}) \AB{𝓥} \AB{Θ}} and {(\AB{Δ} \AR{─Env}) \AB{𝓥} \AB{Θ}}
produces an environment {((\AB{Γ} \AF{++} \AB{Δ}) \AR{─Env}) \AB{𝓥} \AB{Θ})}.

\begin{figure}[h]
\ExecuteMetaData[Generic/Semantics.tex]{body}
\caption{Proof of the Fundamental Lemma of \AR{Sem}antics -- \AF{body}}\label{def:body}
\end{figure}

Given that \AF{fmap} introduces one level of indirection between the
recursive calls and the subterms they are acting upon, the fact
that our terms are indexed by a \AF{Size} is once more crucial in
getting the termination checker to see that our proof is indeed
well founded.

\subsection{Our First Generic Programs: Renaming and Substitution}\label{section:renandsub}

Similarly to ACMM~\citeyear{allais2017type} renaming can be defined generically
for all syntax descriptions as a semantics with \AF{Var} as values and \AD{Tm} as
computations. The first two constraints on \AF{Var} described earlier are trivially
satisfied. Observing that renaming strictly respects the structure of the term it
goes through, it makes sense for the algebra to be implemented using \AF{fmap}.
When dealing with the body of a binder, we `reify' the \AF{Kripke} function by
evaluating it in an extended context and feeding it placeholder values corresponding to
the extra variables introduced by that context. This is reminiscent both of what we
did in Section~\ref{section:primer-program} and the definition of reification in
the setting of normalisation by evaluation (see e.g. Coquand's work~\citeyear{coquand2002formalised}).

Substitution is defined in a similar manner with \AD{Tm} as both
values and computations. Of the two constraints applying to terms as
values, the first one corresponds to renaming and the second
one is trivial. The algebra is once more defined by using \AF{fmap}
and reifying the bodies of binders.

\begin{figure}[h]
\begin{minipage}{0.5\textwidth}
  \ExecuteMetaData[Generic/Semantics.tex]{renaming}
\end{minipage}\hfill
\begin{minipage}{0.5\textwidth}
  \ExecuteMetaData[Generic/Semantics.tex]{substitution}
\end{minipage}
\caption{Generic Renaming and Substitution for All Scope Safe Syntaxes with Binding}
\end{figure}

The reification process mentioned in the definition of renaming
and substitution can be implemented generically for \AR{Sem}antics
families which have \AR{VarLike} values (\AF{vl\textsuperscript{Var}} and \AF{vl\textsuperscript{Tm}}
are proofs of \AR{VarLike} for \AD{Var} and \AD{Tm} respectively) i.e.
values which are thinnable and such that we can craft placeholder values
in non-empty contexts.

\begin{figure}[h]
\ExecuteMetaData[varlike.tex]{varlike}
\caption{\AR{VarLike}: \AF{Thinnable} and with placeholder values}
\end{figure}

For any \AR{VarLike} \AB{𝓥}, we can define \AF{fresh\textsuperscript{r}} of
type {(\AB{Γ} \AR{─Env}) \AB{𝓥} (\AB{Δ} \AF{++} \AB{Γ})} and \AF{fresh\textsuperscript{l}} of
type {(\AB{Γ} \AR{─Env}) \AB{𝓥} (\AB{Γ} \AF{++} \AB{Δ})} by combining the use
of placeholder values and thinnings, and it is almost immediate that variables
are \AR{VarLike}. Hence, we can then write \AF{reify} like so:

\begin{figure}[h]
\ExecuteMetaData[Generic/Semantics.tex]{reify}
\caption{Generic Reification thanks to \AR{VarLike} Values}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% OTHER GENERIC FUNCTIONS


\section{A Catalogue of Generic Programs for Syntax with Binding}

One of the advantages of having a universe of programming language
descriptions is the ability to concisely define an \emph{extension}
of an existing language by using \AD{Desc}ription transformers
grafting extra constructors à la Swiestra~\citeyear{swierstra_2008}.
This is made extremely simple by the
disjoint sum combinator \AF{\_`+\_} which we defined in Section~\ref{desccomb}.
An example of such an extension is the addition of let-bindings to
an existing language.

\subsection{Sugar and Desugaring as a Semantics}\label{section:letbinding}

Let bindings allow the user to avoid repeating themselves by naming
sub-expressions and then using these names to refer to the associated
terms. Preprocessors adding these types of mechanisms to existing
languages (from C to CSS) are rather popular. We introduce a
description of \AD{Let}-bindings which can be used to extend any
language description \AB{d} to \AB{d} \AF{`+} \AF{Let} (where \AF{`+}
is the disjoint of sum of two descriptions defined in Figure~\ref{figure:descsum}):

\begin{figure}[h]
  \ExecuteMetaData[Generic/Examples/ElaborationLet.tex]{letcode}
\caption{Description of a Single Let Binding}
\end{figure}

This description states that a let-binding node stores a pair of types
\AB{$\sigma$} and \AB{$\tau$} and two subterms. First comes the let-bound
expression of type \AB{$\sigma$} and second comes the body of the let which
has type \AB{$\tau$} in a context extended with a fresh variable of type
\AB{$\sigma$}. This defines a term of type \AB{$\tau$}.

In a dependently typed language, a type may depend on a value which
in the presence of let bindings may be a variable standing for an
expression. The user naturally does not want it to make any difference
whether they used a variable referring to a let-bound expression or
the expression itself. Various typechecking strategies can accommodate
this expectation: in Coq~\cite{Coq:manual} let bindings are primitive
constructs of the language and have their own typing and reduction
rules whereas in Agda they are elaborated away to the core language
by inlining.

This latter approach to extending a language \AB{d} with let bindings
by inlining them before typechecking can be implemented generically as
a semantics over (\AB{d} \AF{`+} \AF{Let}). For this semantics values
in the environment and computations are both let-free terms. The algebra
of the semantics can be defined by parts thanks to \AF{case} defined in
Section~\ref{desccomb}: the old constructors are kept the same by interpreting
them using the generic \AF{Substitution} algebra;
whilst the let-binder precisely provides the extra value to be added to the
environment. The process of removing let binders is kickstarted with a
placeholder environment associating each variable to itself.

\begin{figure}[h]
\begin{minipage}{0.45\textwidth}
  \ExecuteMetaData[Generic/Examples/ElaborationLet.tex]{unletcode}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
  \ExecuteMetaData[Generic/Examples/ElaborationLet.tex]{unlet}
\end{minipage}
\caption{Inlining Let Binding}
\end{figure}

In less than 10 lines of code we have defined a generic extension of
syntaxes with binding together with a semantics which corresponds
to an elaborator translating away this new construct. In their
own setting working on STLC, ACMM~\citeyear{allais2017type} have
shown that it is similarly possible to implement a Continuation
Passing Style transformation as a semantics.

We have demonstrated how easily one can define extensions and combine
them on top of a base language without having to reimplement common
traversals for each one of the intermediate representations. Moreover,
it is possible to define \emph{generic} transformations elaborating
these added features in terms of lower-level ones. This suggests that
this setup could be a good candidate to implement generic compilation
passes and could deal with a framework using a wealth of slightly
different intermediate languages à la Nanopass~\cite{Keep:2013:NFC:2544174.2500618}.

\subsection{(Unsafe) Normalisation by Evaluation}\label{section:nbyeval}

A key type of traversal we have not studied yet is a language's
evaluator. Our universe of syntaxes with binding does not impose
any typing discipline on the user-defined languages and as such
cannot guarantee their totality. This is embodied by one of our running
examples: the untyped $\lambda$-calculus. As a consequence there
is no hope for a safe generic framework to define normalisation
functions.

The clear connection between the \AF{Kripke} functional space
characteristic of our semantics and the one that shows up in
normalisation by evaluation suggests we ought to manage to
give an unsafe generic framework for normalisation by evaluation.
By temporarily \textbf{disabling Agda's positivity checker},
we can define a generic reflexive domain \AD{Dm} in which to
interpret our syntaxes. It has three constructors corresponding
respectively to a free variable, a constructor's counterpart where
scopes have become \AF{Kripke} functional spaces on \AD{Dm} and
an error token because the evaluation of untyped programs may go wrong.

\begin{figure}[h]
\ExecuteMetaData[Generic/Examples/NbyE.tex]{domain}
\caption{Generic Reflexive Domain}
\end{figure}

This datatype definition is utterly unsafe. The more conservative
user will happily restrict herself to typed settings where the
domain can be defined as a logical predicate or opt instead for
a step-indexed approach.

But this domain does make it possible to define a generic \AF{nbe}
semantics which, given a term, produces a value in the reflexive
domain. Thanks to the fact we have picked a universe of finitary syntaxes, we
can \emph{traverse}~\cite{mcbride_paterson_2008} the functor to define
a (potentially failing) reification function turning elements of the
reflexive domain into terms. By composing them, we obtain the
normalisation function which gives its name to normalisation by
evaluation.

The user still has to explicitly pass an interpretation of
the various constructors because there is no way for us to
know what the binders are supposed to represent: they may
stand for $\lambda$-abstractions, $\Sigma$-types, fixpoints, or
anything else.


\begin{figure}[h]
\ExecuteMetaData[Generic/Examples/NbyE.tex]{nbe-setup}
\caption{Generic Normalisation by Evaluation Framework}
\end{figure}

Using this setup, we can write a normaliser for the untyped
$\lambda$-calculus: we use \AF{case} from section \ref{desccomb} to distinguish between
the semantical counterpart of the application constructor on
one hand and the $\lambda$-abstraction one on the other.
The latter is trivial: functions are already
values! The semantical counterpart of application proceeds by
case analysis on the function: if it corresponds to a
$\lambda$-abstraction, we can fire the redex by using the Kripke
functional space; otherwise we grow the spine of stuck
applications.

\begin{figure}[h]
\ExecuteMetaData[Generic/Examples/NbyE.tex]{nbelc}
\caption{Normalisation by Evaluation for the Untyped $\lambda$-Calculus}
\end{figure}

We have not used the \AIC{⊥} constructor so \emph{if} the evaluation terminates
(by disabling totality checking we have lost all guarantees of the sort) we know
we will get a term in normal form.

\subsection{An Algebraic Approach to Typechecking}\label{section:typechecking}

Following Atkey~\citeyear{atkey2015algebraic}, we can consider type checking
and type inference as a possible semantics for a bi-directional~\cite{pierce2000local}
language. We represent the raw syntax of a simply typed bi-directional calculus
as a bi-sorted language using a notion of \AD{Mode} to distinguish between terms
for which we will be able to \AIC{Infer} the type and the ones for which we will
have to \AIC{Check} a type candidate.

Following traditional presentations, eliminators give rise to \AIC{Infer}rable
terms under the condition that the term they are eliminating is also \AIC{Infer}rable
and the other arguments are \AIC{Check}able whilst constructors are always \AIC{Check}able.
Two extra constructors allow changes of direction: \AIC{Cut} annotates a \AIC{Check}able
term with its type thus making it \AIC{Infer}rable whilst \AIC{Emb} embeds \AIC{Infer}rables
into \AIC{Check}ables.

\begin{figure}[h]
\begin{minipage}{0.3\textwidth}
  \ExecuteMetaData[Generic/Examples/TypeChecking.tex]{constructors}
\end{minipage}\hfill
\begin{minipage}{0.6\textwidth}
  \ExecuteMetaData[Generic/Examples/TypeChecking.tex]{bidirectional}
\end{minipage}
\caption{A Bidirectional Simply Typed Language}
\end{figure}

The values stored in the environment will be \AD{Type} information for bound
variables no matter what their \AD{Mode} is. In contrast, the generated
computations will, depending on the mode, either take a type candidate and
\AIC{Check} it is valid or \AIC{Infer} a type for their argument. These
computations are always potentially failing so we use the \AD{Maybe} monad.

\begin{figure}[h]
\begin{minipage}{0.40\textwidth}
\ExecuteMetaData[Generic/Examples/TypeChecking.tex]{varmode}
\end{minipage}\hfill
\begin{minipage}{0.50\textwidth}
\ExecuteMetaData[Generic/Examples/TypeChecking.tex]{typemode}
\end{minipage}
\caption{Var- and Type- Relations indexed by the Mode}
\end{figure}

We can now define typechecking as a \AR{Sem}antics. The algebra describes the
algorithm (\AF{\_<\$\_} takes an \AB{A} and a {\AD{Maybe} \AB{B}} and returns
a {\AD{Maybe} \AB{A}} which has the same structure as its second argument):
\begin{itemize}
  \item when facing an application: infer the type of the function, make sure
    it is an arrow type, check the argument at the domain's type and return the
    codomain
  \item for a $\lambda$-abstraction: check the input type is an arrow type and
    check the body at the codomain type in the extended environment where the
    newly-bound variable has the domain's type
  \item a cut always comes with a type candidate against which to check the term
    and to be returned in case of success
  \item finally, the change of direction from \AIC{Infer}rable to \AIC{Check}able
    is successful
    when the inferred type is equal to the expected one.
\end{itemize}

\begin{figure}[h]
\ExecuteMetaData[Generic/Examples/TypeChecking.tex]{typecheck}
\caption{Type- Inference / Checking as a Semantics}
\end{figure}

We have defined a bidirectional typechecker for
this simple language by leveraging the \AF{Sem}antics framework. The code attached
to this paper also contains a variant with more informative types: instead of simply
generating a type or checking that a candidate will do, we can use our \AD{Desc}riptions
to describe a language of evidence and generate not only an expression's type but
also a well scoped and well typed term of that type.

\subsection{Binding as Self-Reference: Representing Cyclic Structures}\label{def:colist}

Ghani, Hamana, Uustalu and Vene~\citeyear{ghani2006representing} have
demonstrated how Altenkirch and Reus' type-level de Bruijn
indices~\citeyear{altenkirch1999monadic} can be used to represent
potentially cyclic structures by a finite object. In their
representation each bound variable is a pointer to the node
that introduced it. Given that we are, at the top-level, only
interested in structures with no ``dangling pointers'', we introduce
the notation \AF{TM} \AB{d} to mean closed terms (i.e. terms of type
\AD{Tm} \AB{d} \AF{∞} \AIC{[]}).

A basic example of such a structure is a potentially cyclic list which
offers a choice of two constructors: \AIC{[]} which ends the list and
\AIC{\_:\!:\_} which combines a head and a tail but also acts as a binder
for a self-reference; these pointers can be used by using the \AIC{var}
constructor which we have renamed \AIC{↶} (pronounced ``backpointer'')
to match the domain-specific meaning.
We can see this approach in action in the examples
\AF{[0, 1]} and \AF{01↺} (pronounced ``0-1-cycle'') which describe
respectively a finite list containing
0 followed by 1 and a cyclic list starting with 0, then 1, and then
repeating the whole list again by referring to the first cons cell
represented here by the de Bruijn variable 1 (i.e. \AIC{s} \AIC{z}).

\begin{figure}[h]
\begin{minipage}{0.55\textwidth}
  \ExecuteMetaData[Generic/Examples/Colist.tex]{clistD}
  \ExecuteMetaData[Generic/Examples/Colist.tex]{clistpat}
\end{minipage}\hfill
\begin{minipage}{0.35\textwidth}
  \ExecuteMetaData[Generic/Examples/Colist.tex]{zeroones}
\end{minipage}
\caption{Potentially Cyclic Lists: Description, Pattern Synonyms and Examples}
\end{figure}

These finite representations are interesting in their own right
and we can use the generic semantics framework defined earlier
to manipulate them. A basic building block is the \AF{unroll}
function which takes a closed tree, exposes its top node and
unrolls any cycle which has it as its starting point. We can
decompose it using the \AF{plug} function which, given a closed
and an open term, closes the latter by plugging the former at
each free \AIC{`var} leaf. Noticing that \AF{plug}'s fundamental nature
is that of substituting a term for each leaf, it makes sense to
implement it by re-using the \AF{Substitution} semantics we already have.

\begin{figure}[h]
\begin{minipage}{0.52\textwidth}
  \ExecuteMetaData[Generic/Cofinite.tex]{plug}
\end{minipage}\hspace{2em}
\begin{minipage}{0.43\textwidth}
  \ExecuteMetaData[Generic/Cofinite.tex]{unroll}
\end{minipage}
\caption{Plug and Unroll: Exposing a Cyclic Tree's Top Layer}
\end{figure}

However, one thing still out of our reach with our current tools
is the underlying co-finite trees these finite objects are meant
to represent. We start by defining the coinductive type
corresponding to them as the greatest fixpoint of a notion of
layer. One layer of a co-finite tree is precisely given by the
meaning of its description where we completely ignore the binding
structure. We show with \AF{01⋯} the infinite list that
corresponds to the example \AF{01↺} given above. The definition
proceeds by copattern-matching as introduced in \cite{abel2013copatterns}
and showcased in \cite{thibodeau2016case}.

\begin{figure}[h]
\begin{minipage}{0.5\textwidth}
  \ExecuteMetaData[Generic/Cofinite.tex]{cotm}
\end{minipage}\hfill
\begin{minipage}{0.4\textwidth}
  \ExecuteMetaData[Generic/Examples/Colist.tex]{zeroones2}
\end{minipage}
\caption{Co-finite Trees: Definition and Example}
\end{figure}

We can then make the connection between potentially cyclic
structures and the co-finite trees formal by giving an \AF{unfold}
function which, given a closed term, produces its unfolding.
The definition proceeds by unrolling the term's top layer and
co-recursively unfolding all the subterms.

\begin{figure}[h]
 \ExecuteMetaData[Generic/Cofinite.tex]{unfold}
\caption{Generic Unfold of Potentially Cyclic Structures}
\end{figure}

Even if the
powerful notion of semantics described in Section~\ref{section:semantics}
cannot encompass all the traversals we may be interested in,
it provides us with reusable building blocks: the definition
of \AF{unfold} was made very simple by reusing the generic
program \AF{fmap} and the \AF{Substitution} semantics whilst
the definition of \AR{∞Tm} was made easy by reusing \AF{⟦\_⟧}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% GENERIC PROOFS


\section{Building Generic Proofs about Generic Programs}

ACMM~\citeyear{allais2017type} have
already shown that, for the simply typed $\lambda$-calculus, introducing an abstract
notion of Semantics not only reveals the shared structure of common
traversals, it also allows them to give abstract proof frameworks for
simulation or fusion lemmas. Their idea naturally extends to our generic
presentation of semantics for all syntaxes.

The most important concept in this section is (\AF{Zip} \AB{d}), a relation
transformer which characterises structurally equal layers such that their
substructures are themselves related by the relation it is passed as an
argument. It inherits a lot of its relational arguments' properties: whenever
\AB{R} is reflexive (respectively symmetric or transitive) so is {\AF{Zip} \AB{d} \AB{R}}.\label{lem:zipstable}

It is defined by induction on the description and case analysis on the two
layers which are meant to be equal:
\begin{itemize}
  \item In the stop token case \AIC{`∎} \AB{i}, the two layers are considered to
    be trivially equal (i.e. the constraint generated is the unit type)
  \item When facing a recursive position {\AIC{`X} \AB{$\Delta$} \AB{j} \AB{d}}, we
    demand that the two substructures are related by {\AB{R} \AB{$\Delta$} \AB{j}}
    and that the rest of the layers are related by \AF{Zip} \AB{d} \AB{R}
  \item Two nodes of type {\AIC{`$\sigma$} \AB{A} \AB{d}} will
    be related if they both carry the same payload \AB{a} of type \AB{A} and if
    the rest of the layers are related by {\AF{Zip} (\AB{d} \AB{a}) \AB{R}}.
\end{itemize}

\begin{figure}[h]
 \ExecuteMetaData[Generic/Zip.tex]{ziptype}
\caption{Zip: Characterising Structurally Equal Values with Related Substructures}
\end{figure}

If we were to take a fixpoint of \AF{Zip}, we could obtain a structural
notion of equality for terms which we could prove equivalent to propositional
equality. Although interesting in its own right, this section will focus
on more advanced use-cases.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SIMULATION


\subsection{Simulation Lemma}\label{section:simulation}

A \AF{Zip} constraint appears naturally when we want to say that a
semantics can simulate another one. Given a relation \AB{$\mathcal{R}^{\mathcal{V}}$} connecting values
in \AB{$\mathcal{V}_1$} and \AB{$\mathcal{V}_2$}, and a relation \AB{$\mathcal{R}^{\mathcal{C}}$} connecting computations in
\AB{$\mathcal{C}_1$} and \AB{$\mathcal{C}_2$}, we can define \AF{Kripke\textsuperscript{R}} relating values
\AF{Kripke} \AB{$\mathcal{V}_1$} \AB{$\mathcal{C}_1$} and \AF{Kripke} \AB{$\mathcal{V}_2$} \AB{$\mathcal{C}_2$}
by stating that they send related inputs to related outputs. We use
the relation transformer \AF{∀[\_]} which lifts a relation on values
to one on environments in a pointwise manner.

\begin{figure}[h]
 \ExecuteMetaData[varlike.tex]{kripkeR}
\caption{Relational Kripke Function Spaces: From Related Inputs to Related Outputs}
\end{figure}

We can then combine \AF{Zip} and \AF{Kripke\textsuperscript{R}} to express the idea
that two semantic objects of respective types
\AF{⟦} \AB{d} \AF{⟧} (\AF{Kripke} \AB{$\mathcal{V}_1$} \AB{$\mathcal{C}_1$}) and
\AF{⟦} \AB{d} \AF{⟧} (\AF{Kripke} \AB{$\mathcal{V}_2$} \AB{$\mathcal{C}_2$}) are
synchronised. The simulation constraint on the algebras for two \AF{Sem}antics 
then becomes: given synchronized objects, the algebras should yield
related computations. Together with self-explanatory constraints on
\ARF{var} and \ARF{th\textsuperscript{$\mathcal{V}$}}, this constitutes the whole \AF{Sim}ulation
constraint:

\begin{figure}[h]
 \ExecuteMetaData[Generic/Simulation.tex]{recsim}
\caption{A Generic Notion of Simulation}
\end{figure}

The fundamental lemma of simulations is a generic theorem showing that for
each pair of \AR{Sem}antics respecting the \AR{Sim}ulation constraint, we
get related computations given environments of related input values. This
theorem is once more mutually proven with a statement about \AF{Scope}s,
and \AD{Size}s play a crucial role in ensuring that the function is indeed total.

\begin{figure}[h]
\begin{minipage}{\textwidth}
  \ExecuteMetaData[Generic/Simulation.tex]{simbody}
\end{minipage}\hfill
\caption{Fundamental Lemma of \AF{Sim}ulations}
\end{figure}

Instantiating this generic simulation lemma, we can for instance get
that renaming and substitution are extensional (given extensionally
equal environments they produce syntactically equal terms), or that
renaming is a special case of substitution. Of course these results
are not new but having them generically over all syntaxes with binding
is convenient; which we have experienced first hand when tackling the
POPLMark Reloaded challenge where \AF{rensub} was actually needed.

\begin{figure}[h]
\ExecuteMetaData[Generic/Simulation.tex]{rensub}
\caption{Renaming as a Substitution via Simulation}
\end{figure}

When studying specific languages, new opportunities to deploy the
fundamental lemma of simulations arise. Our solution to the POPLMark
Reloaded challenge for instance describes the fact that {\AF{sub} \AB{$\rho$} \AB{t}}
reduces to {\AF{sub} \AB{$\rho$'} \AB{t}} whenever for all \AB{v},
\AB{$\rho$}(\AB{v}) reduces to \AB{$\rho$'}(\AB{v}) as a \AR{Sim}ulation.
The main theorem (strong normalisation of STLC via a logical relation)
is itself an instance of (the unary version of) the simulation lemma.

The Simulation proof framework is the simplest examples of the abstract
proof frameworks ACMM~\citeyear{allais2017type} introduce. They also
explain how a similar framework can be defined
for fusion lemmas and deploy it for the renaming-substitution interactions
but also their respective interactions with normalisation by evaluation.
Now that we are familiarised with the techniques at hand, we can tackle
this more complex example for all syntaxes definable in our framework.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% FUSION

\subsection{Fusion Lemma}\label{section:fusion}

Results which can be reformulated as the ability to fuse two traversals
obtained as \AR{Sem}antics into one abound. When claiming that \AF{Tm} is
a Functor, we have to prove that two successive renamings can be fused into
a single renaming where the \AF{Thinning}s have been composed. Similarly,
demonstrating that \AF{Tm} is a relative Monad~\cite{JFR4389} implies proving
that two consecutive substitutions can be merged into a single one whose
environment is the first one, where the second one has been applied in a
pointwise manner. The \emph{Substitution Lemma} central
to most model constructions (see for instance~\cite{mitchell1991kripke}) states
that a syntactic substitution followed by the evaluation of the resulting term
into the model is equivalent to the evaluation of the original term with an
environment corresponding to the evaluated substitution.

A direct application of these results is our (to be published) entry to the
POPLMark Reloaded challenge~\citeyear{poplmarkreloaded}. By using a \AD{Desc}-based
representation of intrinsically well typed and well scoped terms we directly inherit
not only renaming and substitution but also all four fusion lemmas as corollaries
of our generic results. This allows us to remove the usual boilerplate
and go straight to the point.
As all of these statements have precisely the same structure, we can
once more devise a framework which will, provided that its constraints are
satisfied, prove a generic fusion lemma.

Fusion is more involved than simulation so we will step through
each one of the constraints individually, trying to give the reader an intuition
for why they are shaped the way they are.

\subsubsection{The Fusion Constraints}

The notion of fusion is defined for a triple of \AR{Sem}antics; each \AB{$\mathcal{S}_i$}
being defined for values in \AB{$\mathcal{V}_i$} and computations in \AB{$\mathcal{C}_i$}. The
fundamental lemma associated to such a set of constraints will state that
running \AB{$\mathcal{S}_2$} after \AB{$\mathcal{S}_1$} is equivalent to running \AB{$\mathcal{S}_3$} only.

The definition of fusion is parametrised by three relations: \AB{𝓡\textsuperscript{E}} relates
triples of environments of values in {(\AB{$\Gamma$} \AR{─Env}) \AB{𝓥$_1$} \AB{$\Delta$}},
{(\AB{$\Delta$} \AR{─Env}) \AB{𝓥$_2$} \AB{$\Theta$}} and {(\AB{$\Gamma$} \AR{─Env}) \AB{𝓥$_3$} \AB{$\Theta$}}
respectively; \AB{$\mathcal{R}^{\mathcal{V}}$} relates pairs of values \AB{𝓥$_2$} and \AB{𝓥$_3$};
and \AB{$\mathcal{R}^{\mathcal{C}}$}, our notion of equivalence for evaluation results, relates pairs
of computation in \AB{𝓒$_2$} and \AB{𝓒$_3$}.

% \ExecuteMetaData[Generic/Fusion.tex]{fusiontype}

The first obstacle we face is the formal definition of ``running \AB{$\mathcal{S}_2$}
after \AB{$\mathcal{S}_1$}'': for this statement to make sense, the result of running
\AB{$\mathcal{S}_1$} ought to be a term. Or rather, we ought to be able to extract a
term from a \AB{𝓒$_1$}. Hence the first constraint: the existence of a \ARF{quote\textsubscript{1}}
function, which we supply as a field of the record \AR{Fusion}. When dealing with syntactic semantics such as renaming or substitution
this function will be the identity. However nothing prevents to try to prove for
instance that normalisation by evaluation is idempotent in which case a bona fide
reification function extracting terms from model values will be used.

\ExecuteMetaData[Generic/Fusion.tex]{fusionquote}

Then, we have to think about what happens when going under a binder: \AB{$\mathcal{S}_1$}
will produce a \AF{Kripke} function space where a syntactic
value is required. Provided that \AB{$\mathcal{V}_1$} is \AR{VarLike}, we can make use of \AF{reify}
to get a \AF{Scope} back. Hence the second constraint.

\ExecuteMetaData[Generic/Fusion.tex]{fusionvarlike}

Still thinking about going under binders: if three evaluation environments
\AB{$\rho_1$} in {(\AB{$\Gamma$} \AR{─Env}) \AB{𝓥$_1$} \AB{$\Delta$}}, \AB{$\rho_2$} in
{(\AB{$\Delta$} \AR{─Env}) \AB{𝓥$_2$} \AB{$\Theta$}}, and \AB{$\rho_3$} in {(\AB{$\Gamma$} \AR{─Env}) \AB{𝓥$_3$} \AB{$\Theta$}}
are related by \AB{𝓡\textsuperscript{E}} and we are given a thinning \AB{$\sigma$} from \AB{$\Theta$} to \AB{$\Omega$}
then \AB{$\rho_1$}, the thinned \AB{$\rho_2$} and the thinned \AB{$\rho_3$} should still be related.

\ExecuteMetaData[Generic/Fusion.tex]{fusionthinnable}

Remembering that \AF{\_>>\_} is used in the definition of \AF{body} (Figure~\ref{def:body}) to
combine two disjoint environments {(\AB{Γ} \AR{─Env}) \AB{𝓥} \AB{Θ}} and
{(\AB{Δ} \AR{─Env}) \AB{𝓥} \AB{Θ}} into one of type
{((\AB{Γ} \AF{++} \AB{Δ}) \AR{─Env}) \AB{𝓥} \AB{Θ})}, we mechanically need a
constraint stating that \AF{\_>>\_} is compatible with \AB{𝓡\textsuperscript{E}}. We demand
as an extra precondition that the values \AB{$\rho_2$} and \AB{$\rho_3$} are extended
with are related according to \AB{$\mathcal{R}^{\mathcal{V}}$}. Lastly, for all the types to match up,
\AB{$\rho_1$} has to be extended with placeholder variables.

\ExecuteMetaData[Generic/Fusion.tex]{fusionappend}

We finally arrive at the constraints focusing on the semantical counterparts
of the terms' constructors. When evaluating a variable, on the one hand \AB{$\mathcal{S}_1$}
will look up its meaning in the evaluation environment, turn the resulting value into
a computation which will get quoted and then the result will be evaluated with \AB{$\mathcal{S}_2$}.
Provided that all three evaluation environments are related by \AB{𝓡\textsuperscript{E}} this should
be equivalent to looking up the value in \AB{$\mathcal{S}_3$}'s environment and turning it into a
computation. Hence the constraint \ARF{var\textsuperscript{R}}:

\ExecuteMetaData[Generic/Fusion.tex]{fusionvar}

The case of the algebra follows a similar idea albeit being more complex:
a term gets evaluated using \AB{$\mathcal{S}_1$} and to be able to run \AB{$\mathcal{S}_2$}
afterwards we need to recover a piece of syntax. This is possible if the
\AF{Kripke} functional spaces are reified by being fed placeholder \AB{𝓥$_1$} arguments
(which can be manufactured thanks to the \ARF{vl\textsuperscript{𝓥$_1$}} we mentioned before) and
then quoted. Provided that the result of running \AB{$\mathcal{S}_2$} on that term is
related via {\AF{Zip} \AB{d} (\AF{Kripke\textsuperscript{R}} \AB{$\mathcal{R}^{\mathcal{V}}$} \AB{$\mathcal{R}^{\mathcal{C}}$}} to the result
of running \AB{$\mathcal{S}_3$} on the original term, the \ARF{alg\textsuperscript{R}} constraint states
that the two evaluations yield related computations.

\ExecuteMetaData[Generic/Fusion.tex]{fusionalg}

\subsubsection{The Fundamental Lemma of Fusion}

This set of constraint is enough to prove a fundamental lemma of \AR{Fus}ion
stating that from a triple of related environments, one gets a pair of related
computations: the composition of \AB{$\mathcal{S}_1$} and \AB{$\mathcal{S}_2$} on one hand and
\AB{$\mathcal{S}_3$} on the other. This lemma is once again proven mutually with its
counterpart for \AR{Sem}'s \AF{body}'s action on \AR{Scope}s.
% and the \AD{Size} indices make the proof modular.

\begin{figure}[h]
 \ExecuteMetaData[Generic/Fusion.tex]{fusbody}
\caption{Fundamental Lemma of \AR{Fus}ion}
\end{figure}

\subsubsection{Instances of Fusion}

A direct consequence of this result is the four lemmas collectively stating
that any pair of renamings and / or substitutions can be fused together to
produce either a renaming (in the renaming-renaming interaction case) or a
substitution (in all the other cases). One such example is the fusion of
substitution followed by renaming into a single substitution where the
renaming has been applied to the environment.

\begin{figure}[h]
 \ExecuteMetaData[Generic/Fusion.tex]{subren}
\caption{A Corollary: Substitution-Renaming Fusion}
\end{figure}

Another corollary of the fundamental lemma of fusion is the observation that
Kaiser, Schäfer, and Stark~\citeyear{Kaiser-wsdebr} make: \emph{assuming
functional extensionality}, all the ACMM~\citeyear{allais2017type} traversals
are compatible with variable renaming. We can reproduce this result generically
for all syntaxes (see accompanying code) but refrain from using it in practice
when an axiom-free alternative is provable.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BISIMILARITY

\subsection{Definition of Bisimilarity for Co-finite Objects}

Although we were able to use propositional equality when studying
syntactic traversals working on terms, it is not the appropriate
notion of equality for co-finite trees. What we want is a generic
coinductive notion of bisimilarity for all co-finite tree types
obtained as the unfolding of a description. Two trees are bisimilar
if their top layers have the same shape and their substructures are
themselves bisimilar. This is precisely the type of relation \AF{Zip}
was defined to express. Hence the following coinductive relation.

\begin{figure}[h]
 \ExecuteMetaData[Generic/Bisimilar.tex]{bisim}
\caption{Generic Notion of Bisimilarity for Co-finite Trees}
\end{figure}

We can then prove by coinduction that this generic definition always gives
rise to an equivalence relation by using \AF{Zip}'s stability properties
(if \AB{R} is reflexive / symmetric / transitive then so is {\AF{Zip} \AB{d} \AB{R}})
mentioned in Section~\ref{lem:zipstable}.

\begin{center}
%\begin{figure}[h]
 \ExecuteMetaData[Generic/Bisimilar.tex]{eqrel}
% \caption{Bisimilarity is an Equivalence Relation}
% \end{figure}
\end{center}

This definition can be readily deployed to prove e.g. that the unfolding
of \AF{01↺} defined in Section~\ref{def:colist} is indeed bisimilar to \AF{01⋯}
which was defined in direct style. The proof is straightforward due to the simplicity
of this example: the first \AIC{refl} witnesses the fact that both definitions
pick the same constructor (a cons cell), the second that they carry the
same natural number, and we can conclude by an appeal to the coinduction
hypothesis.

\begin{center}
%\begin{figure}[h]
\ExecuteMetaData[Generic/Examples/Colist.tex]{bisim01}
%\caption{The unfolding of \AF{01↺} is bisimilar to the direct-style \AF{01⋯}}
%\end{figure}
\end{center}


%\section{Fully Worked-out Example}
%\todo{system F, etc.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% RELATED WORK

\section{Related Work}\label{section:related-work}

\subsection{Variable Binding} The representation of variable binding
in formal systems has been a hot topic for decades. Part of the purpose
of the first POPLMark challenge~\citeyear{poplmark} was to explore and
compare various methods.

Having based our work on a de Bruijn encoding of variables, and thus a
canonical treatment of \(\alpha\)-equivalence classes, our work has no
direct comparison with permutation-based treatments such as those of
Pitts' and Gabbay's nominal syntax~\cite{gabbay:newaas-jv}.

Our generic universe of syntax is based on
scope-and-typed de Bruijn indices~\cite{de1972lambda} but it is not
a necessity. It is for instance possible to give an interpretation
of \AD{Desc}riptions corresponding to Chlipala's Parametric Higher-Order
Abstract Syntax~\citeyear{chlipala2008parametric} and we would be interested
to see what the appropriate notion of Semantics is for this representation.

\subsection{Alternative Binding Structures} The binding structure we
present here is based on a flat, lexical scoping strategy. There are
other strategies and it would be interesting to see whether
our approach could be reused in these cases.

Bach Poulsen, Rouvoet, Tolmach, Krebbers and Visser~\citeyear{BachPoulsen}
introduce notions of scope graphs and frames to scale the techniques typical
of well scoped and typed deep embeddings to imperative languages. They can
already handle a large subset of Middleweight Java.

We have demonstrated how to write generic programs over the potentially
cyclic structures of Ghani, Hamana, Uustalu and Vene~\citeyear{ghani2006representing}.
Further work by Hamana~\citeyear{Hamana2009} yielded a different presentation
of cyclic structures which preserves sharing: pointers can not only refer
to nodes above them but also across from them in the cyclic tree. Capturing
this class of inductive types as a set of syntaxes with binding and writing
generic programs over them is still an open problem.

\subsection{Semantics of Syntaxes with Binding} An early foundational study
of a general \emph{semantic} framework for signatures with binding, algebras
for such signatures, and initiality of the term algebra, giving rise to a
categorical `program' for substitution and proofs of its properties, was given
by Fiore, Plotkin and Turi~\cite{FiorePlotkinTuri99}, working in the category of presheaves
over renamings, (a skeleton of) the category of finite sets. The presheaf
condition corresponds to our notion of being \AF{Thinnable}. Exhibiting
algebras based on both de Bruijn \emph{level} and \emph{index} encodings,
their approach isolates the usual (abstract) arithmetic required of such encodings.

By contrast, working in an \emph{implemented} type theory, where the encoding
can be understood as its own foundation, without appeal to an external mathematical
semantics, we are able to go further in developing machine-checked such
implementations and proofs, themselves generic with respect to an abstract syntax
\AD{Desc} of syntaxes-with-binding. Moreover, the usual source of implementation
anxiety, namely concrete arithmetic on de Bruijn indices, has been successfully
encapsulated via the \AF{□} coalgebra structure. It is perhaps noteworthy that
our type-theoretic constructions, by contrast with their categorical ones,
appear to make fewer commitments as to functoriality, thinnability, etc. in our
specification of semantics, with such properties typically being \emph{provable}
as a further instance of our framework.

\subsection{Meta-Theory Automation via Tactics and Code Generation} The
tediousness of repeatedly
proving similar statements has unsurprisingly led to various attempts at
automating the pain away via either code generation or the definition of
tactics. These solutions can be seen as untrusted oracles driving the
interactive theorem prover.

Polonowski's DBGen~\citeyear{polonowski:db} takes as input a raw syntax with
comments annotating binding sites. It generates a module defining lifting,
substitution as well as a raw syntax using names and a validation function
transforming named terms into de Bruijn ones; we refrain from calling it a
scopechecker as terms are not statically proven to be well scoped.

Kaiser, Schäfer, and Stark~\citeyear{Kaiser-wsdebr} build on our previous paper
to draft possible theoretical foundations for Autosubst, a so-far untrusted
set of tactics. The paper is based on a specific syntax: well-scoped call-by-value
System F. In contrast, our effort has been here to carve out
a precise universe of syntaxes with binding and give a systematic account
of their semantics and proofs.

Keuchel, Weirich, and Schrijvers' Needle~\citeyear{needleandknot} is a code
generator written in Haskell producing syntax-specific Coq modules
implementing common traversals and lemmas about them.

\subsection{Universes of Syntaxes with Binding} Keeping in mind Altenkirch
and McBride's observation that generic programming is everyday programming
in dependently-typed languages~\citeyear{genericprogramming-dtp}, we can naturally
expect generic, provably sound, treatments of these notions in tools such as
Agda or Coq.

Keuchel, Weirich, and Schrijvers' Knot~\citeyear{needleandknot} implements
as a set of generic programs the traversals and lemmas generated in specialised
forms by their Needle program. They see Needle as a pragmatic choice: working
directly with the free monadic terms over finitary containers would be too cumbersome. In
our experience solving the POPLMark Reloaded challenge, Agda's pattern
synonyms~\cite{Pickering:patsyn} make working with an encoded definition almost
seamless.

The GMeta generic framework~\citeyear{gmeta} provides a universe of syntaxes
and offers various binding conventions (locally nameless~\cite{Chargueraud2012}
or de Bruijn indices). It also generically implements common traversals (e.g. computing
the sets of free variables,
% measuring the size of a term,
shifting
de Bruijn indices or substituting terms for parameters) as well as common
predicates (e.g. being a closed term) and provides generic lemmas proving that
they are well behaved. It does not offer a generic framework
for defining new well scoped-and-typed semantics and proving their properties.

Érdi~\citeyear{gergodraft} defines a universe inspired by a first draft of this
paper and gives three different interpretations (raw, scoped and typed syntax)
related via erasure. He provides scope-and-type
preserving renaming and substitution as well as various generic proofs that
they are well behaved but offers neither a generic notion of semantics, nor
generic proof frameworks.

Copello~\citeyear{copello2017} works with \emph{named} binders and
defines nominal techniques (e.g. name swapping) and ultimately $\alpha$-equivalence
over a universe of regular trees with binders inspired by Morris'~\citeyear{morris-regulartt}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CONCLUSION

\section{Conclusion and Future Work}

Recalling Allais, Chapman, McBride and McKinna's earlier work~\citeyear{allais2017type}
we have started from an example
of a scope-and-type safe language (the simply typed $\lambda$-calculus), have studied
common invariant preserving traversals and noticed their similarity.
After introducing a notion of semantics and refactoring these traversals as
instances of the same fundamental lemma, we have observed the tight
connection between the abstract definition of semantics and the shape of the
language.

By extending a universe of datatype descriptions to support a notion of binding,
we have given a generic presentation of syntaxes with binding as well
as a large class of scope-and-type safe generic programs acting on all of them:
from renaming and substitution, to normalisation by evaluation, and the desugaring
of new constructors added by a language transformer. The code accompanying the
paper also demonstrates how to generically write a printer or a scope-checker
elaborating values of a raw syntax using strings as variable names into scope-safe
ones.

We have seen how to construct generic proofs about these generic programs. We
first introduced a Simulation relation showing what it means for two semantics
to yield related outputs whenever they are fed related input environments. We
then built on our experience to tackle a more involved case: identifying a set
of constraints guaranteeing that two semantics run consecutively can be subsumed
by a single pass of a third one.

We have put all of these results into practice by using them to solve the (to be
published) POPLMark Reloaded challenge which consists of formalising strong
normalisation for the simply typed $\lambda$-calculus via a logical-relation
argument. This also gave us the opportunity to try our framework on larger
languages by tackling the challenge's extensions to sum types and G\"{o}del's
System T.

Finally, we have demonstrated that this formalisation can be re-used
in other domains by seeing our syntaxes with binding as potentially cyclic
terms. Their unfolding is a non-standard semantics and we provide the
user with a generic notion of bisimilarity to reason about them.

% \subsection{Future work}

The diverse influences leading to this work suggest many opportunities
for future research.

Our example of the elaboration of an enriched language to a core one, and ACMM's
implementation of a Continuation Passing Style conversion function raises the question
of how many such common compilation passes can be implemented generically.

An extension of McBride's theory of ornaments~\citeyear{mcbride2010ornamental}
could provide an appropriate framework to highlight the connection between various
languages, some being seen as refinements of others. This is particularly
evident when considering the informative typechecker (see the accompanying
code) which given a scoped term produces a scoped-and-typed term by
type-checking or type-inference.

Our work on the POPLMark Reloaded challenge highlights a need for generic
notions of congruence closure which would come with guarantees (if the original
relation is stable under renaming and substitution so should the closure).
Similarly, the ``evaluation contexts'' corresponding to a syntax could be
derived automatically by building on the work of Huet~\citeyear{huet_1997}
and Abbott, Altenkirch, McBride and Ghani~\citeyear{abbott2005data}, 
allowing us to revisit previous work based on concrete instances of
ACMM such as McLaughlin, McKinna and Stark~\citeyear{craig2018triangle}.

Finally, now knowing how to generically describe syntaxes and their
well behaved semantics, we can start asking what it means to define
well behaved judgments. Why stop at helping the user write their specific
language's meta-theory when we could study meta-meta-theory?

